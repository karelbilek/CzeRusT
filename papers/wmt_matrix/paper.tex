\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

% xelatex
\usepackage{fontspec, xunicode, xltxtra}
\defaultfontfeatures{Mapping=tex-text}
% The following command makes the ASCII quotation mark " (\textquotedbl) available.
% But it disables direct input of accented UTF-8 characters, e.g. "č".
% I do not know how to enable both at the same time.
% \code{"} seems to be the closest match but it will use a different font.
% http://tex.stackexchange.com/questions/43561/how-does-one-display-straight-quotes-in-the-normal-font
%\usepackage[T1]{fontenc} % otherwise the ASCII quotation mark " will not be available
%\setmainfont{Times New Roman}

\usepackage{url}
\def\footurl#1{{\footnote{\url{#1}}}}
\usepackage{color} % pro korektury
\newcommand{\red}[1]{\textcolor{red}{#1}} % komentare (TODO)
\newcommand{\todo}[1]{\textcolor{red}{[TODO: {#1}]}} % komentáře TODO viditelné v textu článku
\newcommand{\rf}{\todo ref}
\newcommand{\XXX}{\textcolor{red}{XXX }} % komentáře (TODO)
\usepackage{verbatim} % pro zakomentování delších kusů textu pomocí \begin{comment} \end{comment}
% Ondřejovy definice
\def\equo#1{``#1''}  % use \equo{something} for quotation marks
\def\code#1{{\tt{}#1}}
\def\flag#1{{\tt{}#1}}
\def\syst#1{{\sc{}#1}}
\def\tilda{\~{}}
\def\amp{\&{}}
\def\parcite#1{\cite{#1}}  % for citation like (Bojar, 2009)
\def\perscite#1{\newcite{#1}} % for citation like Bojar (2009)
\def\Tref#1{Table~\ref{#1}}
\def\Sref#1{Section~\ref{#1}}
\def\Fref#1{Figure~\ref{#1}}

\def\perscite#1{\newcite{#1}}
\def\parcite#1{\cite{#1}}

\title{CUni Multilingual Matrix in the WMT 2013 Shared Task}

%\author{Karel Bílek \hspace{3cm} Daniel Zeman\\
%	Charles University in Prague, Faculty of Mathematics and Physics, \\
%	Institute of Formal and Applied Linguistics\\
%            Malostranské náměstí 25, CZ-11800 Praha, Czechia \\
%	{\tt kb@karelbilek.com, zeman@ufal.mff.cuni.cz}\\
%}

\date{}

% Paper submission deadline   June 7, 2013
% Notification of acceptance June 24, 2013
% End of manual evaluation    June 7, 2013
% Camera-ready deadline       July 5, 2013

\begin{document}
	\maketitle
	\begin{abstract}
We describe our experiments with phrase-based machine translation for the WMT 2013 Shared Task.
We trained one system for 18 translation directions between English or Czech on one side
and English, Czech, German, Spanish, French or Russian on the other side.
We describe a set of results with different training data sizes and subsets. For directions between English and Russian and English and Czech, we describe a set of independent experiments with slightly different translation models.
	\end{abstract}


\begin{comment} %%%TODO
\section{Osnova}
\todo{
\begin{itemize}
\item Quotation marks; our heuristics, official normalization, we do not want it primary for human evaluation
\end{itemize}
}
\end{comment}



\section{Introduction}
\label{sec:intro}

With so many official languages, Europe is a paradise for machine translation research.
One of the largest bodies of electronically available parallel texts is being nowadays generated by the European Union and its institutions.
At the same time, the EU also provides motivation and boosts potential market for machine translation outcomes.

Most of the major European languages belong to one of three branches of the Indo-European language family: Germanic, Romance or Slavic.
Such relatedness is responsible for many structural similarities in European languages, although significant differences still exist.
Within the language portfolio selected for the WMT shared task, English, French and Spanish seem to be closer to each other than to the rest.

German, despite being genetically related to English, differs in many properties.
Its word order rules, shifting verbs from one end of the sentence to the other, easily create long-distance dependencies.
Long German compound words are notorious for increasing out-of-vocabulary rate, which has led many researchers to devising unsupervised compound-splitting techniques.
Also, uppercase/lowercase distinction is more important because all German nouns start with an uppercase letter by the rule.

Czech is a language with rich morphology (both inflectional and derivational) and relatively free word order.
In fact, the predicate-argument structure, often encoded by fixed word order in English, is usually captured by inflection (especially the system of 7 grammatical cases) in Czech.
While the free word order of Czech is a problem when translating to English
(the text should be parsed first in order to determine the syntactic functions
and the English word order),
generating correct inflectional affixes is indeed a challenge for English-to-Czech systems.
Furthermore, the multitude of possible Czech word forms (at least order of magnitude higher than in English) makes the data sparseness problem really severe, hindering both directions.

Most of the above characteristics of Czech also apply to Russian, another Slavic language.
Similar issues have to be expected when translating between Russian and English.
Still, there are also interesting divergences between Russian and Czech, especially
on the syntactic level. Russian sentences typically omit copula and there is also no
direct equivalent of the verb ``to have''. Periphrastic constructions such as
``there is XXX by him'' are used instead. These differences make the Czech-Russian
translation interesting as well. Interestingly enough, results of machine translation between Czech and Russian has so far been worse than between English and any of the two languages, language relatedness notwithstanding.

\begin{comment}
There are numerous ways how these issues could be addressed.
For instance, parsing and syntax-aware reordering of the source-language sentences can help with the word order differences (same goal could be achieved by a reordering model or a synchronous context-free grammar in a hierarchical system).
Factored translation, a secondary language model of morphological tags or even a morphological generator are some of the possible solutions to the poor-to-rich translation issues.
\end{comment}

Our goal is to run one system under as similar conditions as possible to all fourteen translation directions,
to compare their translation accuracies and see why some directions are easier than others.
Future work will benefit from knowing what are the special processing needs for a given language pair.
The current version of the system does not include really language-specific techniques:
we neither split German compounds, nor do we address the peculiarities of Czech and Russian
mentioned above.

In an independent set of experiments, we tried to deal with the data sparseness of Russian language with the addition of a backoff model with a simple stemming and some additional data; those experiments were done for Russian and Czech combinations and English and Russian combinations.


\section{The Translation System}
\label{sec:system}

Both sets of experiments use the same basic framework. The translation system is built around Moses\footnote{\url{http://www.statmt.org/moses/}} \parcite{moses}. Two-way word alignment was computed using GIZA++\footnote{\url{http://code.google.com/p/giza-pp/}} \parcite{giza}, and alignment symmetrization using the \textit{grow-diag-final-and} heuristic \parcite{grow-diag-final-and}. Weights of the system were optimized using MERT \parcite{mert}. No lexical reordering model was trained.

For language modeling we use the SRILM toolkit\footnote{\url{http://www-speech.sri.com/projects/srilm/}} \parcite{srilm} with modified Kneser-Ney smoothing \parcite{kneser-ney,modified-kneser-ney}.

\section{General experiments}
In the first set of experiments we wanted to use the same setting for all language pairs.

\subsection{Data and Pre-processing Pipeline}

We applied our system to all the ten official language pairs. In addition, we also experimented with translation between Czech on one side and German, Spanish, French or Russian on the other side. Training data for these additional language pairs were obtained by combining parallel corpora of the officially supported pairs. For instance, to create the Czech-German parallel corpus, we identified the intersection of the English sides of Czech-English and English-German corpora, respectively; then we combined the corresponding Czech and German sentences.

We took part in the constrained task. Unless explicitly stated otherwise, the translation model in our experiments was trained on the combined News-Commentary v8 and Europarl v7 corpora.\footnote{\url{http://www.statmt.org/wmt13/translation-task.html\#download}}
Note that there is only News Commentary and no Europarl for Russian.
We were also able to evaluate several combinations with large parallel corpora:
the UN corpus (English, French and Spanish), the Giga French-English corpus and CzEng
(Czech-English). We did not use any large corpus for Russian-English.
Tables \ref{tab:newscommentary} and \ref{tab:largeparallel} show the sizes of the training data.

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r r}
Corpus & SentPairs & Tkns lng1 & Tkns lng2\\
\hline
cs-en & 786,929 & 18,196,080 & 21,184,881\\
de-en & 2,098,430 & 55,791,641 & 58,403,756\\
es-en & 2,140,175 & 62,444,507 & 59,811,355\\
fr-en & 2,164,891 & 70,363,304 & 60,583,967\\
ru-en & 150,217 & 3,889,215 & 4,100,148\\
de-cs & 657,539 & 18,160,857 & 17,788,600\\
es-cs & 697,898 & 19,577,329 & 18,926,839\\
fr-cs & 693,093 & 19,717,885 & 18,849,244\\
ru-cs & 103,931 & 2,642,772 & 2,319,611\\
\end{tabular}
\end{center}
\caption{Number of sentence pairs and tokens for every language pair in the parallel training corpus. Languages are identified by their ISO 639 codes: cs = Czech, de = German, en = English, es = Spanish, fr = French, ru = Russian.
Every line corresponds
to the respective version of EuroParl + News Commentary.}
\label{tab:newscommentary}
\end{table}

\begin{table}[h!]
\begin{center}
{\small
\begin{tabular}{l | r r r}
Czeng & SentPairs & Tkns lng1 & Tkns lng2\\
\hline
cs-en & 14,833,358 & 204,837,216 & 235,177,231\\
\hline
UN & & &\\
\hline
es-en & 11,196,913 & 368,154,702 & 328,840,003\\
fr-en & 12,886,831 & 449,279,647 & 372,627,886\\
\hline
Giga & & &\\
\hline
fr-en & 22,520,400 & 854,353,231 & 694,394,577\\
\end{tabular}
}
\end{center}
\caption{Sizes of additional large parallel corpora.}
\label{tab:largeparallel}
\end{table}

The News Test 2010
(2489 sentences in each language) and 2012 (3003 sentences)
data sets\footurl{http://www.statmt.org/wmt13/translation-task.html} were used
as development data for MERT.
BLEU scores reported in this paper were computed on the News Test 2013 set
(3000 sentences each language).
We do not use the News Tests 2008, 2009 and 2011.

All parallel and monolingual corpora underwent the same preprocessing.
They were tokenized and some characters normalized or cleaned.
A set of language-dependent heuristics was applied in an attempt to restore and normalize
the directed (opening/closing) quotation marks (i.e. \code{"}quoted\code{"} $\rightarrow$ ``quoted'').
The motivation is twofold here:
First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses;
second, if Moses learns to output directed quotation marks, subsequent detokenization will be easier.

\begin{comment}
We use a slightly modified tokenization rules compared to CzEng export format.
Most notably, we normalize English abbreviated negation and auxiliary verbs
(\equo{couldn't} $\rightarrow$ \equo{could not}) and attempt at normalizing
quotation marks to distinguish between opening and closing one following proper
typesetting rules.
\end{comment}

The data are then tagged and lemmatized.
We used the Featurama tagger for Czech and English lemmatization and TreeTagger for German, Spanish, French and Russian lemmatization. All these tools are embedded in the Treex analysis framework \parcite{tectomt}.

The lemmas are used later to compute word alignment.
Besides, they are needed to apply  \equo{supervised truecasing} to the data:
we cast the case of the lemma to the form, relying on our morphological
analyzers and taggers to identify proper names, all other words are lowercased.
Note that guessing of the true case is only needed for the sentence-initial token.
Other words can typically be left in their original form, unless they are
uppercased as a form of HIGHLIGHTING.



\subsubsection{Quotation Marks}
\label{sec:quotes}

%%%%%%%%%%
\begin{comment}
< Portugal ' s three Presidencies can best be summed up as ' ambitious on the Community ' s behalf ' , for on each occasion Portugal has tackled vital contemporary issues - the Lisbon Strategy , fo
r example , or the EU ' s relations with Africa and with South America .
---
> Portugal ' s three Presidencies can best be summed up as “ ambitious on the Community ' s behalf ” , for on each occasion Portugal has tackled vital contemporary issues - the Lisbon Strategy , fo
r example , or the EU ' s relations with Africa and with South America .
4308c4310

Spanish directed ? and !

mezery kolem někdy jsou, někdy ne
pravidla pro pořadí (tečka a uvozovka) se liší jazyk od jazyka a situace od situace

gdiff.pl europarl-v6.de-en/en.orig.gz europarl-v6b.de-en/en.orig.gz | less

< " The Commission ' s internal control " he said " is not forceful enough in preventing incorrect operation .
> “ The Commission ' s internal control ” he said “ is not forceful enough in preventing incorrect operation .

gdiff.pl europarl-v6.de-en/en.orig.gz europarl-v6b.de-en/en.orig.gz | grep -P '“[^“”]*“[^“”]*”[^“”]*”'

< As the Wise Men ' s Report also says , and I quote : ' It is elementary ' common sense ' that the Commission should have supported the Parliament ' s decision - making process . '
> As the Wise Men ' s Report also says , and I quote : “ It is elementary “ common sense ” that the Commission should have supported the Parliament ' s decision - making process . ”
\end{comment}
%%%%%%%%%%

A broad range of characters is used to represent quotation marks in the training data:
straight ASCII quotation mark; Unicode directed quotation marks (U+2018 to U+201F);
acute and grave accents; math symbols such as prime and double prime (U+2032 to U+2037)
etc. Spaces around quotes in the original untokenized text ought to provide hints as to the
direction of the quotes (no space between the opening quote and the next word, and
no space between the closing quote and the previous word) but unfortunately there are
numerous cases where superfluous spaces are inserted or required spaces are missing.

Nested quoting is also possible, such as in

\textit{As the Wise Men ' s Report also says , and I quote : ' It is elementary ' common sense ' that the Commission should have supported the Parliament ' s decision - making process . '}

We want all possible quotation marks converted to one pair of characters.
We do not mind the distinction between single and double quotes but we want to keep
(or restore) the distinction between opening and closing quotes.
In addition, we want to identify the apostrophe acting as grapheme
in some languages, and keep it (or normalize it, as it could also be mis-typed
as acute accent or something else):

\textit{As the Wise Men ' s Report also says , and I quote : “ It is elementary “ common sense ” that the Commission should have supported the Parliament ' s decision - making process . ”}

We attempt at solving the problem by a set of rules that consider mutual positions of
quotation marks, spaces and other punctuation, and also some language-dependent
rules (especially on the lexical apostrophe, e.g. in French \textit{d', l'}).

Our rules applied to 1.84~\% of Spanish sentences, 2.47~\% Czech, 2.77~\% German,
4.33~\% English and 16.9~\% French (measured on Europarl data).
%%%TODO: Tohle by se mělo zjistit i pro ruštinu (porovnat normalizovaná data s nenormalizovanými),
%%%TODO: ale teď na to nemám čas ani chuť.

Our approach is different from the normalization script provided and applied by the organizers
of the shared task, which merely converts all quotes to the undirected ASCII characters.
We believe that such MT output is incorrect, so we submitted two versions of each system run:
the \textit{primary} version is intended for human evaluation and does not apply the
``official'' normalization of punctuation. In contrast, the \textit{secondary} version is normalized,
which naturally leads to higher scores in the automatic evaluation.



\subsection{Experiments}
\label{sec:experiments}

In the following section we describe several different settings and corpora combinations we experimented with.
BLEU scores have been computed by our system,
comparing truecased tokenized hypothesis with truecased tokenized reference translation.

Such scores must differ from the official evaluation---see \Sref{sec:final} for discussion of the final results.

The confidence interval for most of the scores lies between ±0.5 and ±0.6 BLEU $\%$ points.



\subsubsection{Baseline Experiments}

The set of baseline experiments were trained on the supervised truecased
combination of News Commentary and Europarl.
As we had lemmatizers for the languages, word alignment was computed on lemmas.
(But our previous experiments showed that there was little difference
between using lemmas and lowercased 4-character ``stems''.)
A hexagram language model was trained on the monolingual version of the
News Commentary + Europarl corpus (typically a slightly larger superset
of the target side of the parallel corpus).



\subsubsection{Larger Monolingual Data}
\label{sec:lm}

Besides the monolingual halves of the parallel corpora, additional monolingual data were provided / permitted:

\begin{itemize}
\item The Crawled News corpus from the years 2007 to 2012, various sizes for each language and year.
\item The Gigaword corpora published by the Linguistic Data Consortium, available only for English ($5^{th}$ edition), Spanish ($3^{rd}$) and French ($3^{rd}$).
\end{itemize}

Our experiments in previous years clearly showed that the Crawled News corpus, in-domain and large, contributed
significantly to better BLEU scores. This year we included it in our baseline experiments
for all language pairs: translation model on News Commentary + Europarl,
language model on monolingual part of the two, plus Crawled News (\texttt{news.all}).

\Tref{tab:mono} gives the sizes of the subsets available for our experiments and \Tref{tab:largelmbleu} compares BLEU scores with Gigaword against the baseline.
Gigaword mainly contains texts from news agencies and as such should be also in-domain.
Nevertheless, the crawled news are already so large that the improvement contributed by Gigaword
is rarely significant.

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r}
Corpus & Segments & Tokens\\
\hline
newsc+euro.cs & 830,904 & 18,862,626\\
newsc+euro.de & 2,380,813 & 59,350,113\\
newsc+euro.en & 2,466,167 & 67,033,745\\
newsc+euro.es & 2,330,369 & 66,928,157\\
newsc+euro.fr & 2,384,293 & 74,962,162\\
newsc.ru & 183,083 & 4,340,275\\
news.all.cs & 27,540,827 & 460,356,173\\
news.all.de & 54,619,789 & 1,020,852,354\\
news.all.en & 68,341,615 & 1,673,187,787\\
news.all.es & 13,384,314 & 388,614,890\\
news.all.fr & 21,195,476 & 557,431,929\\
news.all.ru & 19,912,911 & 361,026,791\\
gigaword.en & 117,905,755 & 4,418,360,239\\
gigaword.es & 31,304,148 & 1,064,660,498\\
gigaword.fr & 21,674,453 & 963,571,174\\
\end{tabular}
\end{center}
\caption{Number of segments (paragraphs in Gigaword, sentences elsewhere) and tokens of additional monolingual training corpora. ``newsc+euro'' are the monolingual versions of the News Commentary and Europarl parallel corpora. ``news.all'' denotes all years of the Crawled News corpus for the given language.}
\label{tab:mono}
\end{table}

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r}
Direction & Baseline & Gigaword\\
\hline
en-cs & 0.1632 & \\
en-de & 0.1833 & \\
en-es & 0.2808 & 0.2856\\
en-fr & 0.2987 & 0.2988\\
en-ru & 0.1582 & \\
cs-en & 0.2328 & 0.2367\\
de-en & 0.2389 & 0.2436\\
es-en & 0.2916 & 0.2975\\
fr-en & 0.2887 & \\
ru-en & 0.1975 & 0.2003\\
\hline
cs-de & 0.1595 & \\
cs-es & 0.2170 & 0.2220\\
cs-fr & 0.2220 & 0.2196\\
cs-ru & 0.1660 & \\
de-cs & 0.1488 & \\
es-cs & 0.1580 & \\
fr-cs & 0.1420 & \\
ru-cs & 0.1506 & \\
\end{tabular}
\end{center}
\caption{BLEU scores of the baseline experiments (left column) on News Test 2013 data, computed by the system on tokenized data, versus similar setup with Gigaword.
The improvement, if any, was typically not significant.}
\label{tab:largelmbleu}
\end{table}




\subsubsection{Larger Parallel Data}
\label{sec:un}

Various combinations with larger parallel corpora were also tested.
We do not have results for all combinations because these experiments needed
a lot of time and resources and not all of them finished in time successfully.

In general the UN corpus seems to be of low quality or too much off-domain.
It may help a little if used in combination with news-euro.
If used separately, it always hurts the results.

The Giga French-English corpus gave the best results for English-French as expected,
even without the core news-euro data. However, training the model on data of this size
is extremely demanding on memory and time.

Finally, Czeng undoubtedly improves Czech-English translation in both directions.
The news-euro dataset is smaller for this language pair, which makes Czeng stand out even more.
See \Tref{tab:unbleu} for details.

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l l l r}
Dir & Parallel & Mono & $BLEU$\\
\hline
en-es & news-euro & +gigaword & 0.2856\\
en-es & news-euro-un & +gigaword & 0.2844\\
en-es & un & un+gigaw. & 0.2016\\
\hline
en-fr & giga & +gigaword & 0.3106\\
en-fr & giga & +newsall & 0.3037\\
en-fr & news-euro-un & +gigaword & 0.3010\\
en-fr & news-euro & +gigaword & 0.2988\\
en-fr & un & un & 0.2933\\
\hline
es-en & news-euro & +gigaword & 0.2975\\
es-en & news-euro-un & baseline & 0.2845\\
es-en & un & un+news & 0.2067\\
\hline
fr-en & news-euro-un & +gigaword & 0.2914\\
fr-en & news-euro & baseline & 0.2887\\
fr-en & un & un+news & 0.2737\\
\end{tabular}
\end{center}
\caption{BLEU scores with different parallel corpora.}
\label{tab:unbleu}
\end{table}



\subsubsection{Final Results}
\label{sec:final}

\Tref{tab:finalbleu} compares our BLEU scores with those computed at \url{matrix.statmt.org}.

$BLEU$ (without flag) denotes BLEU score computed by our system,
comparing truecased tokenized hypothesis with truecased tokenized reference translation.

The official evaluation by \url{matrix.statmt.org} gives typically
lower numbers, reflecting the loss caused by detokenization and
new (different) tokenization.

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r r}
Direction & $BLEU$ & $BLEU_l$ & $BLEU_t$\\
\hline
en-cs & 0.1786 & 0.180 & 0.170\\
en-de & 0.1833 & 0.179 & 0.173\\
en-es & 0.2856 & 0.288 & 0.271\\
en-fr & 0.3010 & 0.270 & 0.259\\
en-ru & 0.1582 & 0.142 & 0.142\\
cs-en & 0.2527 & 0.259 & 0.244\\
de-en & 0.2389 & 0.244 & 0.230\\
es-en & 0.2856 & 0.288 & 0.271\\
fr-en & 0.2887 & 0.294 & 0.280\\
ru-en & 0.1975 & 0.203 & 0.191\\
\hline
cs-de & 0.1595 & 0.159 & 0.151\\
cs-es & 0.2220 & 0.225 & 0.210\\
cs-fr & 0.2220 & 0.191 & 0.181\\
cs-ru & 0.1660 & 0.150 & 0.149\\
de-cs & 0.1488 & 0.151 & 0.142\\
es-cs & 0.1580 & 0.160 & 0.152\\
fr-cs & 0.1420 & 0.145 & 0.137\\
ru-cs & 0.1506 & 0.151 & 0.144\\
\end{tabular}
\end{center}
\caption{BLEU scores with the large language models. $BLEU$ is truecased computed by the system, $BLEU_l$ is the official lowercased evaluation by \url{matrix.statmt.org}. $BLEU_t$ is official truecased evaluation. Although lower official scores are expected, notice the larger gap in en-fr and cs-fr translation. There seems to be a problem in our French detokenization procedure.}
\label{tab:finalbleu}
\end{table}


\subsubsection{Efficiency}
\label{sec:efficiency}

The baseline experiments were conducted mostly on 64bit AMD Opteron quad-core 2.8~GHz CPUs with 32~GB RAM
(decoding run on 15 machines in parallel)
and the whole pipeline typically required between a half and a whole day.

However, we used machines with up to 500~GB RAM to train the large language models and translation models.
Aligning the UN corpora with Giza++ took around 5 days.
Giga French-English corpus was even worse and required several weeks to complete.
Using such a large corpus without pruning is not practical.

\section{Experiments for Russian}
In a separate set of experiments, we tried to take a basic Moses framework and change the setup a little for better results on morphologically rich languages.

Tried combinations were Russian-Czech and Russian-English.

\subsection{Data}
For systems involving Russian, we used following parallel data:

For Russian to Czech translation, we used combination of
\begin{itemize}
\item UMC 0.1 \parcite{umc} - tri-parallel set, consisting of news articles  - 93,432 sentences
\item data, mined from movie subtitles (described in further detail below) - 2,324,373 sentences
\item Czech-Russian part of InterCorp - a corpus from translation of fiction books \parcite{cermak:rosen:10} - 148,847 sentences
\end{itemize}
For Russian to English translation, we used combination of
\begin{itemize}
\item UMC 0.1 - 95,540 sentences
\item subtitles - 1,790,209 sentences
\item Yandex English-Russian parallel corpus \footnote{\url{https://translate.yandex.ru/corpus?lang=en}} - 1,000,000 sentences
\item wiki headlines from WMT website \footnote{\url{http://www.statmt.org/wmt13/translation-task.html}} - 514,859 sentences
\item common crawl from WMT website - 878,386 sentences

\end{itemize}

Added together, Russian-Czech parallel data consisted of 2,566,615 sentences and English-Czech parallel data consisted of 4,275,961 sentences \footnote{some sentences had to be removed for technical reasons}.

We also used 765 sentences from UMC003 as a devset for MERT training.

For monolingual data for Russian-Czech and Russian-English system, we used the following monolingual corpora:

For Russian:
\begin{itemize}
\item Russian sides of all the parallel data - 4,275,961 sentences
\item News commentary from WMT website - 150,217 sentences
\item News crawl 2012 - 9,789,861 sentences
\end{itemize}

For Czech:
\begin{itemize}
\item Czech sides of all the parallel data - 2,566,615 sentences
\item Data, downloaded from Czech news articles\footnote{\url{http://thepiratebay.sx/torrent/7121533/}} - 1,531,403 sentences
\item WebColl \parcite{webcoll} - 4,053,223 sentences
\item PDT \footnote{\url{http://ufal.mff.cuni.cz/pdt2.0/}} - 115,844 sentences
\item Complete Czech Wikipedia - 3,695,172 sentences
\item Sentences scraped from Czech social server okoun.cz - 580,249 sentences
\end{itemize}

For English:
\begin{itemize}
\item English sides of all the paralel data - 4,275,961 sentences
\item News commentary from WMT website - 150,217 sentences
\end{itemize}

\Tref{tab:russdata} and \Tref{tab:russdatam} shows the sizes of the training data.

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r r}
Corpus & SentPairs & Tokens lng1 & Tokens lng2\\
\hline
cs-ru & 2,566,615 & 19,680,239 & 20,031,688 \\
en-ru & 4,275,961 & 64,619,964 & 58,671,725 \\
\end{tabular}
\end{center}
\caption{Number of sentence pairs and tokens for every language pair.}
\label{tab:russdata}
\end{table}
\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r }
Corpus & Sentences & Tokens \\
\hline
en mono& 13,426,211  & 278,199,832   \\
ru mono& 13,701,213 & 231,076,387   \\
cs mono& 12,542,506 & 202,510,993  
\end{tabular}
\end{center}
\caption{Number of sentences and tokens for every language.}
\label{tab:russdatam}
\end{table}

\subsubsection{Tokenization, tagging}

%tokenizaci provedly taggery... nevim, jak to napsat dobre
Czech and English data was tokenized and tagged using Morče tagger; Russian was tokenized and tagged using TreeTagger. TreeTagger also does lemmatization; however, we didn't use lemmas for alignment or translation models, since our experiments showed that primitive stemming got better results.

However, what is important to mention is that TreeTagger had problems with some corpora, mostly Common Crawl. For some reason, Russian TreeTagger has problems with "dirty" data - sentences in English, French or random non-unicode noise. It either slows down significantly or stop working at all. For this reason, we wrapped TreeTagger in a script, that detected those hangs and replaced the errorneous Russian sentences with bogus, one-letter Russian sentences (we can't delete those, since the lines already exist in the opposite languages; but since the pair doesn't really make sense in the first place, it doesn't matter as much).

All the data are lowercased for all the models and we recase the letters only at the very end.

%pozn.: uvozovky jsem absolutně neřešil.
%Spoustu věcí, co mám výše i níže napsané jako fakta ("zkusili jsme titulky" apod.) by to chtělo asi víc exaktně, ale neměl jsem na to nějak čas :-(
\subsubsection{Subtitle data}
For an unrelated project dealing with movie subtitles translation, we obtained data from OpenSubtitles.org for Czech and English subtitles. However, those data were not aligned on sentence level and were more unstructured - we had thousands of .srt files with some sort of metadata.

When exploiting the data from the subtitles, we made several observations:
\begin{itemize}
\item language, used in subtitles, is very different from the language, used in news articles% and the data generally doesn't help that much
\item one of the easiest and most accurate sentence alignments in movie subtitles is the one based purely on the time stamps
\item allowing bigger differences in the time stamps in the alignment produced more data, but less accurate
\item the subtitles are terribly out of domain (as experiments with using \emph{only} the subtitle data showed us), but adding the corpus mined from the subtitles \emph{still} increases the accuracy of the translation
\item allowing bigger differences in the time stamps and, therefore, more (albeit less accurate) data always led to better results in our tests.
\end{itemize}

In the end, we decided to pair as much subtitles as possible, even with the risk of some being misaligned, because we found out that it helps us the most in the end.


\subsection{Translation model, language model}
For alignment, we used primitive stemming, that takes just first 6 letters from a word. We found out that using this "brute force" stemming - for reasons that will have to be explored in a further research - return better results than regular lemmatization, for both alignment and translation model, as described further.

For each language pair, we used a translation model with two translation tables, one of them being as backoff model. More exactly, the primary translation is from a form to a combination of (lower case) form and tag, and the secondary backoff translation is from a "stem" described above to a combination of (lower case) form and tag.

We built two language models - one for tags and one for lower case forms.

The models were actually a mixed model using interpolate option in SRILM - we trained a different language model for each corpus, and then we mixed the language models using a small development set from UMC003.

%Unlike with previous set of experiments, we didn't 

\subsection{Final Results}

The final results from  \url{matrix.statmt.org} are in the table \Tref{tab:finalbleukarel}. You might notice a sharp difference between lowercased and truecased BLEU - that is due to a technical error that we didn't notice before the deadline. %Jsem blby, vsimnul jsem si toho fakt az ted :((

\begin{table}[htbl]
\begin{center}
\begin{tabular}{l | r r}
Direction & $BLEU_l$ & $BLEU_t$ \\
\hline
ru-cs & 0.158 & 0.135 \\
cs-ru & 0.165 & 0.162 \\
ru-en & 0.224 & 0.174 \\
en-ru & 0.163 & 0.160 \\
\end{tabular}
\end{center}
\caption{Lowercased and cased BLEU scores}
\label{tab:finalbleukarel}
\end{table}

\section{Conclusion}
\label{sec:concl}

We have described two independent Moses-based SMT systems we used for the WMT 2013 shared task. We discussed experiments with large data for many language pairs from the point of view of both the translation accuracy and efficiency.

\section*{Acknowledgements}
\label{sec:ack}

% funding Dan
The work on this project was supported by the grant
P406/11/1499 of the Czech Science Foundation (GAČR).

% funding Karel
The work on this project was supported by the grant
639012 of the Grant Agency of Charles University (GAUK).


\bibliographystyle{acl}
\bibliography{paper}
\end{document}
