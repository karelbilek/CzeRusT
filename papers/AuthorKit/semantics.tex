%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
%\usepackage{todonotes}

%\newcommand{\todofnfootnote}{}
\newcommand{\todofn}[1] {
 \footnote{\textbf{TODO : #1}}}

\usepackage{url}
\frenchspacing
\pdfinfo{
/Title (Experiments in maching learning for automatic semantic feature assignment) %
/Subject (AAAI Publications)
/Author (AAAI Press)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%

\title{Experiments in maching learning for automatic semantic feature assignment}
\author{Karel Bilek, Natalija Kljueva}
\maketitle
\begin{abstract}
In this paper we explore supervised machine learning techniques for
assigning semantic categories to nouns. We operate with 16 semantic categories 
relevant especially for the task of words sense disambiguation. The training
data presents a set of 2,783 nouns with assigned semantic categories.   
The central problem of this research is to find the features for the machine learning 
that produce better results in the face of a small trainig data size. 
We exploited the context of a word which is a generally used technique for this task, 
and we also tried the morphological segments as features.

\end{abstract}


\section{Introduction}

Lexicons enhanced with semantic information are frequently used 
in various NLP applications, such as machine translation, question-answering
or sentiment analysis. Probably the most well known resource of such kind is 
WordNet (\cite{wordnet}), the lexicon of words
interlinked by semantic relations and organized hierarchically into 
semantic classes. It nowadays exists for many languages including Czech. Although it is a large scale resource providing complex semantic information, it's applicability is often limited by the fact that it was created by means of a translation of the English WordNet and that it uses a system of categories which may not fit a particular application.

Many additional tools for automatic semantic annotation have been created so far, as, e.g., 
for clustering\todofn{really clustering?} words into semantic classes(e.g., \cite{baroni:2009} -- - for German and Italian) or for 
semantic relation assignment \cite{peirsman} etc.  
 
%Assigning semantic features to words category assignment belongs to one 
%or classification of words into semantic fields
%is exploited in development of ontologies or various NLP applications such as
%word sense disambiguation or question-answering.

In this paper we describe experiments with automatic assignment of semantic features (categories) to Czech nouns
exploiting an existing resource (a small hand-annotated lexicon created originally for a machine translation system). The assignment is performed carried by means of logistic regression models. The model is trained in a supervised manner, using 
%using a small training set of nouns that are annotated with semantic categories. 
%We are excperimenting with 
basically two kinds of features for machine learning --  morphological and syntactic (context behaviour) properties and their combination. The experiments give an answer to a question which type of features is more useful for the given purpose.   


%As features of machine learning 
%\footnote{Initially, in this context of a Machine Learning the notion 'feature' is used, but we will use 'atribute' instead so that it does not interfere with 'semantic feature' concept}.
%we have chosen both 
%The work has been carried out for Czech language.

%Recently many tools for automatic semantic annotation have been created, like
%clustering \todofn{really clustering?} words into semantic classes(\cite{baroni:2009} - for German and Italian), 
%semantic relation assignment \cite{peirsman}. The most well-known resource
%in the field WordNet \cite{wordnet} presents the lexicon of words
%interlinked by semantic relations and organized hierarchically into 
%semantic classes. 

\subsection{Note on notation}
Since the word ``feature'' is frequently used both in linguistic context as a term for semantic category and in 
machine learning context as a term for describing any observable property of a learning example, for 
better clarity, we use the term ``category'' for the notion of semantic feature, 
while we use the word ``feature'' purely for machine learning feature in the rest of this paper.


\section{Motivation}
\subsection{Semantic categories}
Semantic categories are generally viewed as components of meaning
that express one definite sense of a word, they are generally associated
with the contrastive context, so they occur either with plus or minus
sign, ex. [+human], [-time] etc. 
There is no generally accepted set of semantic features, as researches
usually use their own classification depending on their goals. Assigning 
semantic features to concrete words is influenced by a general issue 
of finding the proper level of granularity - if the number of semantic features is relatively 
low, they are not rich enough to capture more subtle differences in the woird meaning;
if too many of them are used, it is more difficult to assign them correctly
for each particular word. This is probably one of the reasons why the 
number of distinct semantic features varies in various experiments 
from only a few (like
the division of nouns into 'animated' or 'non-animated') to the very
fine-grained meaning classification (as, e.g., in WordNet).

Semantic categories can also be used for some minor research problems, as, for example
resolving nominative-accusative ambiguity in Slavic languages.
In \cite{principled_disam} the authors show the way to disambiguate adjectives
with the help of semantic categories of nouns they modify. 

For example, it may help 
to discriminates two senses of the  adjective \textit{short}
: applied to those nouns with the sem. category [+human] versus in combination
with a [+interval] noun. The phrase \textit{a short girl} is translated
into Czech as \textit{mal\'{a} holka}, whereas \textit{a short day} is \textit{kr\'{a}tk\'{y} den}.

It can also help to disambiguate different senses of verbs:\\ 
(1)\textit{The dog runs after the owner.} - [+human] category of a subject\\
(2) \textit{The program runs on Linux.} -[+computer] category of a subject \\
%The latter verb is translated into Czech with the same verb \textit{be(et}
for both senses, so this concrete ambiguity is not relevant for
the translation between English and Czech. On the other hand, If translated into Russian, the verb needs to be translated differently in different contexts: while the verb in (1) will can be translated straightforwardly as \textit{be\v{z}a\v{t}} into Russian,
the metaphorical meaning in (2) must be expressed by another verb -- \textit{rabota\v{t}} - to work.
%More examples on how the semantic categories work will be presented in the next section
%after the lexicon description. \todofn{Will it?}

\subsection{Classification}
The task of semantic category assignment can be naturaly represented as a task of classication, using supervised machine learning algorithms.

Generally, in supervised machine learning, we have a set of training data with defined 
categories and we want to build an algorithm, which will generalize from this training data and return a category for any yet unseen data.

More specifically, in most of the algorithms, the training data can be broken into
\textit{examples}, and each of the examples can be described as a set of \textit{features} and a given \textit{category}. The machine learning algorithm is usually a model with variable parameters, which are then learned from the training data.

With the data we have at our disposal, we can take the semantically annotated words from the lexicon as training examples 
and their linguistic categories as features in machine learning.


\section{Sources}
\subsection{Semantic categories}
The neccessary condition for the success of any supervised machine learning algorithm are reliable training data. Instead of creating a new set of semantic categories and subsequently undertaking a long and costly process of manual annotation, we have decided to re-use existing high quality manually annotated data. Such data exist in the form of a bilingual dictionary of a machine translation system RUSLAN, a rule based MT system translating from Czech to Russian.

The history of RUSLAN \cite{oliva1989parser} goes back to the second half of the 80's when there was a need for an automatic translation of operating systems manuals. The project was abandoned in 90's when
there was no need for the MT between Czech and Russian. Since then, the resources used in the project served mainly as a source of data for other projects, for
example, in \cite{mt-recycled} authors tried to re-use the module
of syntactic analysis of Czech for the Czech-English Machine Translation,
the paper \cite{pisa2010} describes the extraction of morphosyntactic information from the bilingual dictionary of RUSLAN for Czech and Russian valency dictionaries.

%One of the parts of the system \todofn{really?} was a dictionary with added 
The bilingual dictionary of RUSLAN constitutes a rich source of various kinds of data for both Czech and Russian. Apart from the target language equivalents and their morphology, the dictionary provides  morphological, syntactic and semantic information for the source language (Czech), namely declension patterns, valency frames, semantic features etc. In the current experiment we ignore the Russian (target language)
part of the dictionary and we use (a relevant part of) the Czech side, namely the semantic features assigned to all nouns in the dictionary. 

The major drawback of the dictionary of Ruslan for all kinds of experiments is its domain. The project aimed at the translation of manuals to operating systems of mainframes. It thus contains a relatively smaller number of domain-independent words, the domain related expressions prevail. On top of that, even the computer terminology has changed during the past 25 years, so  some of the words contained in the dictionary are a bit outdated. 
That alone caused us bigger problems than we anticipated.

\subsubsection{Mining data from RUSLAN dictionary}

The dictionary of RUSLAN contains about 8,000 entries, 2,783 of which are nouns with assigned semantic categories.

Following is an example of an entry:

\begin{verbatim}
LE2KAR3==MZ(@(*H),!,MA0111,VRAC2).
\end{verbatim}

\begin{itemize}
\item \texttt{LE2KAR3} represents the Czech lemma \emph{l\'{e}ka\v{r}}; the diacritics is encoded in a rather primitive way corresponding to the time when it was created and when encoding of national characters still constituted a big problem.
\item \texttt{MZ} represents a declension pattern (and thus also determines the part of speech information because this particular declension pattern is used for masculine animate nouns in Czech).
\item \texttt{@(*H)} represents the semantic category `animated'.
\item \texttt{MA0111,VRAC2} represents the declension class of the Russian equivalent and the equivalent itself, encoded into basic ASCII
\end{itemize}

From this format we have extracted the Czech side of the dictionary together with the semantic categories. 
When we took the categories without ``sanity checking'' and filtering out the possible mistakes, 
we ended up with 2,783 words and 29 categories; however, some of these categories appear only with one or two words, therefore they are not relevant for our purpose. When we filtere out those categories, that don't appear in at least 30 words, we ended up with the features described in Table 1.

\begin{table}
\begin{tabular}{|l|l|l|}
 \hline
\textbf{Shortcut} &  \textbf{Count} & \textbf{Category}\\
A & 941 & abstract \\ \hline
C & 835 & activity \\ \hline
R & 728 & result \\ \hline
K & 712 & concrete \\ \hline
V & 205 & property \\ \hline
H & 165 & animated \\ \hline
Z & 101 & machinery \\ \hline
M & 64 & measure\\ \hline
P & 56 & program \\ \hline
N & 44 & instrument \\ \hline
F & 41 & function \\ \hline
D & 32 & action \\ \hline

\end{tabular}
\caption{Semantic categories in training data}
\end{table}


The total number of categories assigned to words is bigger than the number of words because some words have more than one semantic category. In average, each words is assigned 1.4 categories.


%\subsubsection{Semantic categories and their role in Ruslan}
%\todofn{Následující dve( kapitoly vu*bec nechápu. Ve(r(ím ti, e jsou OK. Karel} 
%There are 29 semantic categories in Ruslan, and each word can have more than one
%feature assigned. In our work we will use only few of them - *H(animated), *A(abstract),
%*K(concrete), *INS(institution), *INT(interval) etc. present in the above table.
%The semantic categories were introduced mainly for disambiguation of verbs.
%Verbs are assigned by the arguments which have restriction on nouns with definite semantic prperties. In example (3)
%the subject of the verb 'to coordinate' can be [+human] or [+institution], whereas
%the object should have the semantic categories either [+concrete] or [+abstract]. The sign (-)
%before the set of features shows that those features are prohibited to used in this context.
%Following is the example of an entry:
%(3)koordinovat(n(+(*h,*ins,*z,*os)),a(+(*a,*k),-(*h,*ins)),koordinirovat').

%We will show now on a rather frequent verb 'to come to' \footnote{The more frequent is the verb the more likely it is ambiguous} 
%how semantic information can influence the disambiguation process.\\
%The ambiguous verb \textit{dojít}that has a literal sense of \textit{going somewhere}
%and both metaphorical meaning \textit{to happen} can be translated properly
%due to the semantic categories assigned to its actants. In (4cz) the
%agent has the semantic category (*H) - animated, so the verb is translated
%in its literal meaning (in Russian 'dojti'), whereas in (4cz) the actant does not have this feature
%and it is translated as 'proizojti':

%(4cz) DOJDE +(*H,*Z,*P,*F),K(D,DO(G)))),DOJTI):\\
%\textit{De(da nemu*e dojít do ordinace sám.}\\
%lit. \textit{Granny couldn't go to hospital on his own}\\

%(5cz) DOJDE (K(D(*C,*A),N)),PROIZOJTI)\footnote{We should mention, that in this example
%not only the semantics is taken into account, but also the surface realization of an argument}.\\
%K této situaci nemu*e dojít\\
%lit. To this situation couldn't come\\
%'Such a situation could not happen'

%\subsubsection{Semantic features - negative experience}
Before we'll introduce additional sources of data, let us make a short remark concerning the role of semantic features in the original system. The main reason why they were introduced was the endeavor to reduce the ambiguity of translation (the system tended to slightly overgenerate the results) and to disallow sentences which would not be semantically plausible. The practical experience from testing clearly showed that using the features for restriction of certain is not a good idea, they tended to be too restrictive and actually blocked the translation more often than they helped to resolve the ambiguity.
%As it was stated in \cite{KubonPHD2001} the semantic features did not
%served the purpose of disambiguation, sometimes they introduced mistakes
%so that translation process failed to produce a result at all. This
%happened especially when a valency frame contained the prohibition
%to use words with definite semantic features.

%\todofn{????? nerozumim - tak to vymazeme,to neni podstatne}We will not use the prohibited features for our prposes, they will be rather "recommended" on the basis of
%statistics.

\subsection{Monolingual data corpus}
Because one of our methods was supposed to exploit an immediate context, it was necessary to use additional annotated corpora. As the highest quality annotation of Czech is provided in the Prague Dependency Treebank (PDT),\footnote{\url{http://ufal.mff.cuni.cz/pdt2.0/}}, it was a natural choice.   
% data sourcea context, we As one of our features, we decided to use the context informations, taken from a monolingual corpus.


\subsubsection{PDT}
PDT 
%- Prague Dependency Treebank \url{http://ufal.mff.cuni.cz/pdt2.0/} - 
represents a collection of Czech texts annotated on three
levels - morphological, analytical(surface syntactic) and tectogrammatical(deep syntactic). It   
contains 115,844 sentences from newspapers and journals.

However, due to the very specific dictionary domain, we have found out that the size of PDT does not provide a sufficient coverage of the words from the RUSLAN domain. 
%We expected most the words to appear at least once in 
The 115,844 sentences of PDT contain 1,957,247 tokens, but out of the 2,783 words from RUSLAN, 813 don't appear at all in the entire corpus, 162 appear only once and 
1,408 words appear less then 10 times.

These numbers clearly indicate that the contextual information based on manually annotated data from RUSLAN is too sparse for machine learning. In would actually mean that many feature vectors would be empty 
%for about half of the examples in the test 
%set, 
which would cause 
%half of the 
many examples to be misaligned completely.

For that reason, we decided to use a bigger corpus with only morphological annotation.

\subsubsection{WebColl}

WebColl\cite{webcoll} is a corpus of texts in Czech crawled from the Czech web, cleaned and annotated  with a POS tagger and lemmatized.
WebColl consists of 7,148,630 sentences, which together have 114,867,064 tokens. 

Although this corpus is about 100 times bigger than PDT, its data cover our lexicon only slightly more. Out of the 2,783 words, 412 don't appear at all, 
40 appear exactly once and 611 words appear less than 10 times. In other words -- increasing the training data size approximately 100 - times results in the removal of only about a half of the unseen words.

The manual revision of the unseen words revealed, that most of those words are very domain specific 
(words such as ``\emph{rebasing}'', ``\emph{subroutine}'', ``\emph{self-relocability}'' and so on) 
and that they probably won't see the context no matter how big corpus we take. With regard to the rest, some of those words were genuine mistakes and some of them were affected by slightly different lemmatization in used RUSLAN and WebColl (incompatible lemmas).


\section{Machine Learning features}
The success of machine learning algorithms to great extent depends on the 
%One of the major decisions for us was 
choice of proper 
features.
% for the machine learning
In our experiments we have tried to exploit two types of features 
%The features we used in our work are
%of two types 
- context ones and morphological ones. 
Roughly speaking, we exploited syntax and morphology in order to learn semantics of a word.  

\subsection{Context}
As J. R. Firth stated ``Words shall be known by the company they keep'',
therefore our first idea (and the whole reason of using a large mono-lingual corpus in the first place) 
was to look at the context in which the words appear and to try to convert it to machine learning features.

The context can in principle be exploited in a number of different ways. For example, in \cite{baroni:2009}, the authors use a method of automatic clustering,  
where semantic relations have been retrieved from context. 
In \cite{biemann05} the semantic features of nouns were learned
from a context, but only adjectives were taken into account.
We have decided to use all parts of speech as a context in our experiment. This decision was motivated by the endeavor to use as much information from the context as possible. 

\todofn{Pokusil jsem se to prepsat, ale nevim, jestli jsem to spravne pochopil VK}
We have taken into account the context of two words to the left and two words to the right. 
First of all, it was necessary to determine the context of all ``known'' words from the dictionary, i.e. the words whose semantic categories we know.  

For every word from the lexicon, we 
have looked at all words in its context.

If $n$ types are found, then it actually means that we have obtained $n$ separate 
features for that particular word, where the value of each feature represents 
the number of times when the feature word appeared in the context.
 
The machine learning model is, then, trained on these features. To assign a category
to an unseen word, we have to go through the entire corpus, count
the features for the unsees word as well using the same algorithm as for the features of the known words. However, this time we are not collecting all words appearing in the left or right context of the unseen word, but only those appearing among the features collecting for known words (in other words, we count all the words in the context and then perform an 
intersection of the counted words and the words from the features). Again, the number obtained for each feature is a value of that feature for the unseen word and becomes a part of the feature vector of the machine learning model.

This naive approach has several drawbacks. Most importantly, our number of features explodes, while the values (number of appearences) themselves are very unevenly distributed.

For this reason we made several corrections:
\begin{itemize}
    \item we nominate a word from context as a feature only when the given context word was seen in at least some fixed number of training examples $min$ - it is not going to help us too much 
if only few known words have this partticular word as a context (a ``training example'' here means a word from the lexicon)
    
    \item we normalize the numbers, so that the features are all about 
the same size. We originally wanted to use percents as values - meaning, 
we wouldn't have the number of appearences as a feature, but the percentage of how often is a given word in the context of the training example.    
     However, this resulted in very small numbers, since actually, 
in most of the training examples, even the top context features are in the order of 
tenths of percents; so, at the end we use integer value percent times ten. 
We ignore the values when the integer value is smaller than 1 (that means, if it is the context in less than 1/1000 of cases).
     

\end{itemize}

If we set $min$ as 40, the number of features is 1,502.

\subsection{Morphology}
As you will see further, experiments with context were not that successful. 
We then tried to experiment with a morphology, altough we initially saw it as just a ``baseline'' model.

Because in Czech language, the ending can often determine the semantic category, 
we decided to try adding endings as a machine learning feature. 
More exactly, for $n=$ 4,3,2 and even 1, we took the last $n$ letters from 
a word and we then created new feature for every such ending and set is as 1.

It actually means that the feature vector for any training example will be mostly zeroes 
(it will be 1 only for one ending of length 4, only 1 for ending of length 3, and so on, 
and the rest will be 0). On the other hand, it is much easier to find such feature 
for a new word, because you just look at the word itself and you don't need to dig through the dictionary.

As with the previous features, the number quickly explodes. We repeat the first measure 
to stop feature explosion - we use only those endings as features that have at least $min$ words in the lexicon ending with these endings.

If we put $min$ as 5, the number of features is 433.

\subsection{Combination}
We also tried to combine the two approaches.

Aside from just putting all the features together, we tried to include the morphology of the context. 
That means - we tried to add features as ``the count of the combinations of last 3 letters of the words 2 on the right'', and so on.

The results were disappointing, though, as will be described further. 
We think this is because the feature space becomes unrealistically big and the so-called Curse of Dimensionality starts to take place.\footnote{This is sometimes known as ``Hughes effect'', see \cite{hughes:1968}} Basically, we get huge matrix with more features than training examples, but most of the matrix is empty.

We tried some simple algorithms for feature reduction, but we didn't get very far with those.

\section{Machine Learning appropaches}
\subsection{Logistic Regression}
For our first approach, we got heavily inspired by the Andrew Ng Machine Learning MOOC  on Coursera.org.

\todofn{Mělo tu celá tato kapitola být? Kdo chce, může si o tom zjistit něco víc. Naopak já jsem tam určitě nasekal spoustu chyb.}Let's suppose that the feature vector is called $y$ and the category, that we want to predict, is called $x$.

In binary logistic regression model with \emph{one category} (meaning - the classifier just predicts 0 or 1), the classifier learns a vector $\theta$, which is as long as the input feature vector. The vector $\theta$ is then multiplied with the feature vector $y$, which gives us value $z$. We then take the value $z$ and apply the so-called sigmoid function or logistic function ($\frac{1}{1+e^{-z}}$), which then gives us a number $h_\theta(x)$. That number is chance that the example \textbf{is} in a given category. We can then output 1 if the chance is higher than some number $\gamma$ (usually 0.5).

The vector $\theta$ is learned using the so-called \emph{gradient descent}. Gradient descent defines so-called cost function for a given theta and one given example with category $y$ (1 or 0) as $-y \log(h_\theta(x))-(1-y)-\log(1-h_\theta(x))$ and average cost function over all the training examples as $J(\theta)$. We then try to find $\min_\theta J(\theta)$ by initializing $\theta$ as some random vector and then periodically updating $\theta:=\theta'$, where $\theta'_i=\theta_i-\alpha\frac{\partial}{\partial \theta_j}J(\theta)-\frac{\lambda}{m}\theta_j$ with a pre-set $\alpha$, count of training examples $m$ and so-called regularization factor $\lambda$.

If we have several binary classifiers, Ng recommends using \emph{one-vs-all} models, with the category with the biggest chance ``winning''. However, this model is only applicable to the case where we have \textbf{\emph{single category}} with every training case. 
In our case, though, we have multiple categories (also called multiple labels in literature). 

We employed a simple solution, where every category has its own classifier trained separately, returning 1 or 0, indicated 
belonging or not belonhing to the category. The item is then indicated as belonging to all the categories where it returned 1.

For every experiment, we put aside the same (at the beginning randomly selected) set for testing purposes. 
On the training set, we put aside a heldout set, and we train the parameters $\lambda$ and $\gamma$ on the heldout set, simply by training the model with the given $\lambda$ and $\gamma$, counting the F-score and getting the best $\lambda$ and $\gamma$ for the given classifier.

These parameters vary from classifier to classifier; for a given feature set, we train 14 classifiers (one for every semantic category), together with training $\lambda$ and $\gamma$ parameters, and then we test those categories on the test set.

%NAKONEC JSEM VYŠKRTL ÚPLNĚ
%\subsection{Mulan}
%\todofn{???}???možná vyškrtnem úplně, pokusy jsem nestihnul, navíc by to zabralo asi až moc času
%
%WEKA (short for ``Waikato Environment for Knowledge Analysis'') is a general framework for machine translation, developed at the University of Waikato.
%
%We couldn't use WEKA directly, because the algorithms that it provides are not compatible with multi-labeled sets such as ours. \todofn{citace z FAQ}
%
%However, we later found out \todofn{přidat citace ze stránky Mulan projektu} that (ten týpek z řecka) have developed a system, 
%based on WeKA, supporting multi-labeled machine learning, called Mulan.
%
%Mulan has not only implemented various machine learning algorithms that we can directly leverage, but it also already 
%covers evaluation for multi-labeled machine learning, which is by itself a difficult task that we will talk about later.
%
%We used some of the algorithms that Mulan provides, together with 10-fold cross-validation that Mulan automatically provides.

\section{Machine Learning evaluation}
Because we use multi-labeled classification, we cannot use only precision and recall. Instead, we use so-called micro-average and macro-average of both precision and recall, and then make their mean average for micro-averaged and macro-averaged F-score. We use $\mu$ as short for micro and $M$ as short for macro.

Micro- and macro-averaged precision and recall is defined as \footnote{From Data Mining wiki  \url{http://datamin.ubbcluj.ro/wiki/index.php/Evaluation_methods_in_text_categorization}, Babes-Bolyai University, Romania} 

$$P_{\mathrm{\mu}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FP_{i}}; \quad R_{\mathrm{\mu}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FN_{i}}$$

$$P_{\mathrm{M}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FP_{i}};\quad R_{\mathrm{M}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FN_{i}}$$

where $TP_i$ is true positive given a category $i$ and $C$ is the set of categories. Basically, micro-averaging gives equal weights to every document, while macro-averaging gives equal weights to every category.

\section{Results}
All the results are rounded to two significant digits.
\subsection{Logistic regression}
\subsubsection{Random baseline}

For comparison, we began with two ``baseline'' algorithms - first was random classifier, that didn't look at the features at all; the second was logistic regression model, but learned on randomly generated 200 columns of random ``features''.

% v resultech - první micro, potom macr
%\begin{table}
\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Rand. class. & 0.11 & 0.51 & 0.19 & 0.11 & 0.57 & 0.18 \\ \hline
Rand. feats. & 0.30 & 0.36 & 0.32 & 0.10 & 0.14 & 0.12 \\ \hline
\end{tabular}
%\end{table}

\subsubsection{Morphological features}
When we take the features as described above, we got these results:


\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Morphology & 0.57 & 0.66 & 0.61 & 0.46 & 0.41 & 0.43 \\ \hline
\end{tabular}

If we break it into individual classifiers, we got these results


\begin{tabular}{|r|r|}
\hline
A & 0.57 \\ \hline
    C & 0.75 \\ \hline
    R & 0.66 \\ \hline
    K & 0.58 \\ \hline
    V & 0.78 \\ \hline
    H & 0.57 \\ \hline
    Z & 0.11 \\ \hline
    M & 0.18 \\ \hline
    P & 0 \\ \hline
    N & 0 \\ \hline
    F & 0 \\ \hline
    D & 0.40 \\ \hline

\end{tabular}

You can see that we got a 0 F-score with several categories. This unfortunately happens, because there is very little examples for those categories in the \emph{testing} data and they all got misaligned.

\subsubsection{Context}
If we take as a context feature 1 word on the left and 1 word on the right, and $min$ set as 40, we have 980 features. With those, results are as follows:


\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Dist. 1 & 0.27 & 0.57 & 0.36 & 0.23 & 0.48 & 0.30 \\ \hline
\end{tabular}


If we take words with distance 2 (with $min$ still 40), we got 1,501 features. The results are as follows:

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Dist. 2 & 0.28 & 0.62 & 0.39 & 0.23 & 0.49 & 0.32 \\ \hline
\end{tabular}

We can see that the results are not that much better.

\subsubsection{Combination - endings and beginnigs of context words}
What we can do now is add the ``combination'' of morphology and context - that is, 
we can try to add endings and beginnings of the words on the left and on the right.

We originally wanted to let the $min$ value 40 to keep the experiments ``consistent'',
but the feature space exploded quickly. So, as a way of a very primitive feature cutting,
we increased the $min$ value for the features where we take more letters.

If we take the endings and beginnings of context words with the distances 1, we have these results:

(We took $min$ as 40 for the lengths 1, 80 for the length 2 and 140 for the length 3. 
Those numbers are quite arbitrarily chosen to keep the feature space relatively small, 
so the algorithms don't become too slow and the feature space too big.)

%\todo{najít ??}
\begin{table}


\begin{tabular}{|r|r|r|r|r|r|r|r|}
\hline
lets.&feats & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
1 & 138 & 0.22 & 0.47 & 0.30 & 0.20 & 0.43 & 0.29 \\ \hline
1, 2  & 901 & 0.25 & 0.54 & 0.35 & 0.22 & 0.48 & 0.30 \\ \hline
1, 2, 3  & 1659 & 0.26 & 0.54 & 0.35 & 0.23 & 0.48 & 0.31 \\ \hline

\end{tabular}
\caption{Combination context and endings}
\end{table}
As you can see, with the added 3rd letter, we are not getting much better and 
the feature space is growing, making our algorithm slower. We are also still worse than with the context alone.


We can wonder if the reason, why the context is not working as 
well as we would want, is that the feature space is simply too big. 
However, even with reducted space, we never got better results from this `
`combined'' way than from pure context; that means, that all we can get out of context 
is best done by looking at the context alone, not at its endings or beginnings.

\subsubsection{Combination - putting features together}
We thought that with combined feature from context and morphology, the approaches would complement 
themselves and the results would get better, than either of them (especially morphology).

However, this didn't turn out to be true - the results are very slightly worse, than with morphology alone, 
further underlining the fact that the morphology of the words, at least in our case and with our technique, 
is more useful than context.

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Combined & 0.54 & 0.70 & 0.61 & 0.33 & 0.42 & 0.37 \\ \hline
\end{tabular}


\subsection{Suffixes and a semantic category}
%It is a known fact, that
%In (Kirchner?) it was stated, that 
One of the linguistically valuable side-effects of our research can be 
the set of endings of words that are most likely to belong to the
respective semantic category.
The form of a word can to some extent signalize the semantic class of it. 
For example, 'er' or 'ee' denote [+human, profession] semantic category of a word.
Below are the examples of the endings that
can indicate the semantic features of words according to our experiments.

\begin{table}[!h]
\begin{tabular}{ll}
\hline
A abstract & \textit{ivost, ekce, ování, íra, ita, nictví}\\
H animated & \textit{átor, ovník, atel, atelka, stník, or, ík}\\
C activity & \textit{ení, ování, ání, ace}\\
D action  & \textit{ení, ání, ace}\\
R result & \textit{ ita, ení, utí, ání, akce, utí}\\
K concrete & \textit{ka, íč, énko, ora}\\
M measure/unit & \textit{etnost, ita, ví, o, kost, etí}\\
V property & \textit{ita, nost, ce}\\
\hline
\end{tabular}
\caption{Sem.categories and the endings they take most frequently}
\end{table}


\section{Conclusion}
In this paper we described machine learning algorithms
created to guess the semantic category of a noun based on the following features:\\
1.Context surroundings of a word\\
2.Morphological characteristics of a word\\
We have shown, that for this concrete task the context properties of a word
does not appear to be promising at least in such a simple manner we
used(regarding context as a word after and a word before) .
Probably, more non-trivial context features should be taken into account
- such as ex. part-of-speech tag.
On the other hand, such simple method as guessing semantic features on the
basis of the surface form of the word brought more favorable %positive?promising?
results in terms of precision. It was also an interesting observation, that
adding context features to that morphological to the machine learning
negatively influenced the overall quality.

The possible future directions of our work will lie both in the field of
optimizing features for the Machine Learning algorithm as well as
adapting WordNet as training and test data for our task.

\bibliographystyle{aaai} \bibliography{biblio}


\end{document}
