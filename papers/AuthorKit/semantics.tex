%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{todonotes}
\usepackage{url}
\frenchspacing
\pdfinfo{
/Title (Experiments in maching learning for automatic semantic feature assignment) %
/Subject (AAAI Publications)
/Author (AAAI Press)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%

\title{}
%\author{}
\maketitle
\begin{abstract}
\begin{quote}
\end{quote}
\end{abstract}


\section{Introduction}

Semantic category assignment or classification of words into semantic fields
is exploited in development of ontologies or various NLP applications such as
word sense disambiguation or question-answering.

In this paper we present our results in experiments with automatically assigning semantic features,
using logistic regression models. The model is trained in a supervised manner
using a small training set of nouns that are annotated with semantic categories.
As features of machine learning 
%\footnote{Initially, in this context of a Machine Learning the notion 'feature' is used, but we will use 'atribute' instead so that it does not interfere with 'semantic feature' concept}.
we have chosen both morphological and syntactic (context behaviour) properties 
of a noun as features for machine learning.
%The work has been carried out for Czech language.

Recently many tools for automatic semantic annotation have been created, like
clustering \todo{really clustering?} words into semantic classes(\cite{baroni:2009} - for German and Italian), 
semantic relation assignment \cite{peirsman}. The most well-known resource
in the field WordNet \cite{wordnet} presents the lexicon of words
interlinked by semantic relations and organized hierarchically into 
semantic classes. 

\subsection{Note on notation}
Since the word ``feature'' is used both in linguistic context as a term for semantic category and in 
machine learning context as a term for describing any observable property of a learning example, for 
better clarity, we use the term ``category'' for the notion of semantic feature, 
while we use the word ``feature'' purely for machine learning feature.


\section{Motivation}
\subsection{Semantic features}
Semantic features are generally viewed as components of meaning
that express one definite sense of a word. They are generally associated
with the contrastive context, so they occur either with plus or minus
sign, ex. [+human], [-time] etc.

There is no general set of semantic features, as researches
use their own classification depending on their goals. The
number of semantic features can vary from only a few components (like
devision of nouns into 'animated' or 'non-animated') to the very
fine-grained meaning classification like in WordNet. Semantic features
are therefore closely related to the ontology creating.

They can be also used for some minor research problems, as, for example
resolving nominative-accusative ambiguity in Slavic languages.
In \cite{principled_disam} the authors show the way to disambiguate adjectives
with the help of semantic categories of nouns they modify. 

For example, 
this discriminates several senses of the  adjective \textit{short}
: applied to those nouns with the sem. feature [+human] versus in combination
with a [+interval] noun. The phrase \textit{short girl} is translated
into Czech as \textit{mala holka}, whereas \textit{short day} is \textit{kratky den}.

This can be also applied to disambiguation of senses of verbs:\\ 
(1)\textit{The dog runs after the owner.} - [+human] feature of a subject\\
(2) \textit{The program runs on Linux.} -[+computer] feature of a subject \\
The latter verb is translated into Czech with the same verb \textit{běžet}
for both senses, so this concrete ambiguity is not relevant for
the translation between English and Czech. On the other hand, in Russian
it might cause a mistake: while the verb in (1) will be still \textit{běžat'} in Russian,
the metaphorical meaning in (2) is expressed by another verb - \textit{rabotat'} - to work.
More examples on how the semantic features work will be presented in the next section
after the lexicon description.

\subsection{Classification}
We realized that the task of semantic category assignment can easily be written as a task of classication, 
using supervised machine learning algorithms.

Generally, in supervised machine learning, we have a set of training data with defined 
categories and we want to build an algorithm, which will generalize from this training data and return a category for any yet unseen data.

More specifically, in most of the algorithms, the training data can be broken into
\textit{examples}, and each of the examples can be described as a set of \textit{features} and a given \textit{category}. The machine learning algorithm is usually a model with variable parameters, which are then learned from the training data.


With our data, we can take the words from the lexicon as training examples 
and their linguistic categories as features in machine learning.



\section{Sources}
\subsection{Semantic categories}
For the machine learning, we first needed a training set, annotated with semantic categories. 
We decided to use the training set from the RUSLAN project.


RUSLAN was a project for automatic, rule-based translation from Czech to Russian. 
Its creation \cite{oliva1989parser} goes back to the 80's when the Rule-Based 
methods in Machine Translation prevailed.  The project was abandoned in 90's when
there was no need for the MT between Czech and Russian. There were
several attempts to exploit the data and the parts of the system, for
example, in \cite{mt-recycled} authors tried to use the module
of syntactic analysis for Czech for the Czech-English Machine Translation
and \cite{pisa2010} extracted the morphosyntactic information for
valency dictionary.

%One of the parts of the system \todo{really?} was a dictionary with added 
The core of the MT system RUSLAN was a dictionary with rich linguistic annotation.
for the rule-based translation. We ignore the Russian 
part of the dictionary and use purely the Czech side. We extracted the 
semantic categories from the dictionary. 

Before we describe the data further, what is needed to be said about the lexicon is 
that the words are from a very special domain - the aim of the translation 
and, therefore, the domain of the lexicon are mainframe computer manuals. 
That alone caused us bigger problems than we anticipated.

\subsubsection{Mining data from RUSLAN dictionary}

Dictionary entries in Ruslan contain morphological,
syntactic and semantic information. The dictionary contains about 8,000 entries, 2,783 of which are nouns
with assigned semantic categories.

Following is the example of an entry:

\begin{verbatim}
LE2KAR3==MZ(@(*H),!,MA0111,VRAC2).
\end{verbatim}

\begin{itemize}
\item \texttt{LE2KAR3} represents the Czech lemma \emph{lékař}; th
e diacritics is encoded using numbers, since in the 80's, the computers didn't have the encoding options as we have today.
\item \texttt{==MZ} represents part of speech(noun) and declension class.
\item \texttt{@(*H)} represents the semantic category `animated'.
\item \texttt{MA0111,VRAC2} represents the declination class of Russian lemma + lemma itself - 
again, Russian alphabet couldn't be used, so the word is encoded into basic ASCII
\end{itemize}

From this format we extract the Czech side of the dictionary together with the semantic categories. 
When we took the categories without ``sanity checking'' and filtering out the possible mistakes, 
we end up with 2,783 words and 29 categories; however, some of these appear only with one or two words. 
When we filter out those categories, that don't appear with at least 30 words, we end up with those categories:

\begin{table}
\begin{tabular}{|l|l|l|}
 \hline
\textbf{Shortcut} &  \textbf{Count} & \textbf{Category}\\
A & 941 & abstract \\ \hline
C & 835 & activity \\ \hline
R & 728 & result \\ \hline
K & 712 & concrete \\ \hline
V & 205 & property \\ \hline
H & 165 & animated \\ \hline
Z & 101 & machinery \\ \hline
M & 64 & measure\\ \hline
P & 56 & program \\ \hline
N & 44 & instrument \\ \hline
F & 41 & function \\ \hline
D & 32 & action \\ \hline

\end{tabular}
\caption{Semantic categories in training data}
\end{table}


As you can see, the sum is bigger than the number of words; on average, each words has 1.4 categories.


\subsubsection{Semantic features and their role in Ruslan}
%\todo{Následující dvě kapitoly vůbec nechápu. Věřím ti, že jsou OK. Karel}There are 
There are 29 semantic features in Ruslan, and each word can have more than one
feature assigned. In our work we will use only few of them - *H(animated), *A(abstract),
*K(concrete), *INS(institution), *INT(interval) etc. present in the above table.
The semantic categories were introduced mainly for disambiguation of verbs.
Verbs are assigned by the arguments which have restriction on nouns with definite semantic prperties. In example (3)
the subject of the verb 'to coordinate' can be [+human] or [+institution], whereas
the object should have the semantic features either [+concrete] or [+abstract]. The sign (-)
before the set of features shows that those features are prohibited to used in this context.
Following is the example of an entry:
(3)koordinovat(n(+(*h,*ins,*z,*os)),a(+(*a,*k),-(*h,*ins)),koordinirovat').

We will show now on a rather frequent verb 'to come to' \footnote{The more frequent is the verb the more likely it is ambiguous} 
how semantic information can influence the disambiguation process.\\
The ambiguous verb \textit{dojít}that has a literal sense of \textit{going somewhere}
and both metaphorical meaning \textit{to happen} can be translated properly
due to the semantic features assigned to its actants. In (4cz) the
agent has the semantic feature (*H) - animated, so the verb is translated
in its literal meaning (in Russian 'dojti'), whereas in (4cz) the actant does not have this feature
and it is translated as 'proizojti':

(4cz) DOJDE +(*H,*Z,*P,*F),K(D,DO(G)))),DOJTI):\\
\textit{Děda nemůže dojít do ordinace sám.}\\
lit. \textit{Granny couldn't go to hospital on his own}\\

(5cz) DOJDE (K(D(*C,*A),N)),PROIZOJTI)\footnote{We should mention, that in this example
not only the semantics is taken into account, but also the surface realization of an argument}.\\
K této situaci nemůže dojít\\
lit. To this situation couldn't come\\
'Such a situation could not happen'

\subsubsection{Semantic features - negative experience}
As it was stated in \cite{KubonPHD2001} the semantic features did not
served the purpose of disambiguation, sometimes they introduced mistakes
so that translation process failed to produce a result at all. This
happened especially when a valency frame contained the prohibition
to use words with definite semantic features.

%\todo{????? nerozumim - tak to vymazeme,to neni podstatne}We will not use the prohibited features for our prposes, they will be rather "recommended" on the basis of
%statistics.


\subsection{Monolingual data corpus}
As one of our features, we decided to use the context informations, taken from a monolingual corpus.

However, as indicated in the previous section, we got into problems because of the strange lexicon domain.
\subsubsection{PDT}
PDT - Prague Dependency Treebank \url{http://ufal.mff.cuni.cz/pdt2.0/} - is a collection of Czech texts annotated on three
levels - morphological, analytical and tectogrammatical.   
PDT contains 115,844 sentences from newspapers and journals.

We expected most the words to appear at least once in the 115,844 sentences of 1,957,247 tokens. 
However, out of the 2,783 words, 813 don't appear at all and 162 appear exactly once. 
1,408 words appear less then 10 times.

What this means that if we take only context in PDT as a ML feature, in almost 
any test set about half of the words won't even be in the monolingual corpus. 
That would cause empty feature vectors for about half of the examples in the test 
set, which would cause half of the examples to be misaligned completely.

For that reason, we decided to try a bigger corpus.

\subsubsection{WebColl}

WebColl\cite{webcoll} is a corpus of texts in Czech crawled from the Czech web, cleaned and annotated  with a POS tagger and lemmatized.

WebColl consists of 7,148,630 sentences, which together have 114,867,064 tokens. 

This data covers our lexicon a bit better. Out of the 2,783 words, 412 don't appear at all, 
40 appear exactly once and 611 words appear less than 10 times. We have a monolingual 
corpus approximately 100 - times bigger, but we removed only about a half of the unseen words.

We decided, after manually reviewing the words, that those words are very domain specific 
(words such as ``\emph{rebasing}'', ``\emph{subroutine}'', ``\emph{self-relocability}'' and so on) 
and they probably won't see the context no matter how big corpus we take. 
Also, some of those were genuine mistakes and some of those were caused by different lemmatization in RUSLAN and WebColl.


\section{Machine Learning features}
One of the major decisions for us was chosing the proper 
features for the machine learning. The features we used in our work are
of two types - 'Context features' and 'Morphological features'. 
Roughly speaking, we exploited syntax and morphology in order to learn semantics of a word.  

\subsection{Context}
As J. R. Firth stated ``Words shall be known by the company they keep'',
and our first idea (and the whole reason of finding a mono-lingual corpus in the first place) 
was to look at the context in which the words appear and try to convert it to machine learning features.

This is  the generally used method of automatic clustering now, ex. \cite{baroni:2009},
where semantic relations were retrieved from a context as well. 
In \cite{biemann05} the semantic features of nouns were learned
from a context, but only adjectives were taken into account.
In our experiment we use all part of speech as a context. 

We tried to count the words, appearing on the position 1 and 2 words left and 
right from the words, and use the counts as a feature type. 
%For example, assuming we have
%a sentence chunk with a word from training data with a semantic category [animated]:'the \textbf{visitor}[anim] looked'.
%The statistics of the corpus shows that it is very probable to meet the animated noun in this context.
%Our assumption was that a noun on the left from '\textit{looked}' is highly probable to have a category [anim].

Let us describe it in more detail for, for example, \todo{To je divne napsane} position 1 left. For every word in the lexicon, we 
look at all the words, appearing left from them. If $n$ such words appear, we will then have $n$ separate 
features for our words, where the value of the feature would be the word count.
 
The machine learning model is, then, trained on these features. To assign a category
to another unseen word, we would then have to go through the entire corpus, count
the features (which would basically mean to count all the words on the left and then doing an 
intersection of the counted words and the words from the features) and then entering the 
counts as a feature vector to the machine learning model.

This naive approach has several drawbacks. Most importantly, our number of features explodes, while the counts themselves are very unevenly distributed.

For this, we did several corrections:
\begin{itemize}
    \item we take the feature only when the given context word was seen in at least some 
fixed number of training examples $min$ - it is not going to help us too much 
if only few words have this word as a context (what is meant here 
as ``training example'' is a word from lexicon)
    
    \item we normalize the numbers, so that the features are all about 
the same size. We originally wanted to use percents as numbers - meaning, 
we wouldn't have the counts as a feature, but the percent of how often is a given word a context of the training example.
    
     However, then the numbers got very small, since actually, 
in most of the training examples, even the top context features are in order of 
tenths of percents; so we use integer value percent times ten. 
We ignore the values when the integer value is smaller than 1 (that means, if the context is context in less than 1/1000 of cases).
     

\end{itemize}

If we put $min$ as 40, the number of features is 1,502.

\subsection{Morphology}
As you will see further, experiments with context were not that successful. 
We then tried to experiment with a morphology, altough we initially saw it as just a ``baseline'' model.

Because in Czech language, the ending can often determine the semantic category, 
we decided to try adding endings as a machine learning feature. 
More exactly, for $n=$ 4,3,2 and even 1, we took the last $n$ letters from 
a word and we then created new feature for every such ending and set is as 1.

It actually means that the feature vector for any training example will be mostly zeroes 
(it will be 1 only for one ending of length 4, only 1 for ending of length 3, and so on, 
and the rest will be 0). On the other hand, it is much easier to find such feature 
for a new word, because you just look at the word itself and you don't need to dig through the dictionary.

As with the previous features, the number quickly explodes. We repeat the first measure 
to stop feature explosion - we use only those endings as features that have at least $min$ words in the lexicon ending with these endings.

If we put $min$ as 5, the number of features is 433.

\subsection{Combination}
We also tried to combine the two approaches.

Aside from just putting all the features together, we tried to include the morphology of the context. 
That means - we tried to add features as ``the count of the combinations of last 3 letters of the words 2 on the right'', and so on.

The results were disappointing, though, as will be described further. 
We think this is because the feature space becomes unrealistically big and the so-called \todo{reference!!} curse of dimensionality starts to take place. Basically, we get huge matrix with more features than training examples, but most of the matrix is empty.

We tried some simple algorithms for feature reduction, but we didn't get very far with those.

\section{Machine Learning appropaches}
\subsection{Logistic Regression}
For our first approach, we got heavily inspired by the Andrew Ng Machine Learning MOOC  on Coursera.org.

\todo{Mělo by to tu vůbec být? Kdo chce, může si o tom zjistit něco víc.}Let's suppose that the feature vector is called $y$ and the category, that we want to predict, is called $x$.

In binary logistic regression model with \emph{one category} (meaning - the classifier just predicts 0 or 1), the classifier learns a vector $\theta$, which is as long as the input feature vector. The vector $\theta$ is then multiplied with the feature vector $y$, which gives us value $z$. We then take the value $z$ and apply the so-called sigmoid function or logistic function ($\frac{1}{1+e^{-z}}$), which then gives us a number $h_\theta(x)$. That number is chance that the example \textbf{is} in a given category. We can then output 1 if the chance is higher than some number $\gamma$ (usually 0.5).

The vector $\theta$ is learned using the so-called \emph{gradient descent}. Gradient descent defines so-called cost function for a given theta and one given example with category $y$ (1 or 0) as $-y \log(h_\theta(x))-(1-y)-\log(1-h_\theta(x))$ and average cost function over all the training examples as $J(\theta)$. We then try to find $\min_\theta J(\theta)$ by initializing $\theta$ as some random vector and then periodically updating $\theta:=\theta'$, where $\theta'_i=\theta_i-\alpha\frac{\partial}{\partial \theta_j}J(\theta)-\frac{\lambda}{m}\theta_j$ with a pre-set $\alpha$, count of training examples $m$ and so-called regularization factor $\lambda$.

If we have several binary classifiers, Ng recommends using \emph{one-vs-all} models, with the category with the biggest chance ``winning''. However, this model is only applicable to the case where we have \textbf{\emph{single category}} with every training case. 
In our case, though, we have multiple categories (also called multiple labels in literature). 

We employed a simple solution, where every category has its own classifier trained separately, returning 1 or 0, indicated 
belonging or not belonhing to the category. The item is then indicated as belonging to all the categories where it returned 1.

For every experiment, we put aside the same (at the beginning randomly selected) set for testing purposes. 
On the training set, we put aside a heldout set, and we train the parameters $\lambda$ and $\gamma$ on the heldout set, simply by training the model with the given $\lambda$ and $\gamma$, counting the F-score and getting the best $\lambda$ and $\gamma$ for the given classifier.

These parameters vary from classifier to classifier; for a given feature set, we train 14 classifiers (one for every semantic category), together with training $\lambda$ and $\gamma$ parameters, and then we test those categories on the test set.

\subsection{Mulan}
\todo{???}???možná vyškrtnem úplně, pokusy jsem nestihnul, navíc by to zabralo asi až moc času

WEKA (short for ``Waikato Environment for Knowledge Analysis'') is a general framework for machine translation, developed at the University of Waikato.

We couldn't use WEKA directly, because the algorithms that it provides are not compatible with multi-labeled sets such as ours. \todo{citace z FAQ}

However, we later found out \todo{přidat citace ze stránky Mulan projektu} that (ten týpek z řecka) have developed a system, 
based on WeKA, supporting multi-labeled machine learning, called Mulan.

Mulan has not only implemented various machine learning algorithms that we can directly leverage, but it also already 
covers evaluation for multi-labeled machine learning, which is by itself a difficult task that we will talk about later.

We used some of the algorithms that Mulan provides, together with 10-fold cross-validation that Mulan automatically provides.

\section{Machine Learning evaluation}
Because we use multi-labeled classification, we cannot use only precision and recall. Instead, we use so-called micro-average and macro-average of both precision and recall, and then make their mean average for micro-averaged and macro-averaged F-score.

Micro- and macro-averaged precision and recall is defined as \todo{citace \url{http://datamin.ubbcluj.ro/wiki/index.php/Evaluation_methods_in_text_categorization}} 

$$P_{\mathrm{micro}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FP_{i}}; \quad R_{\mathrm{micro}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FN_{i}}$$

$$P_{\mathrm{macro}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FP_{i}};\quad R_{\mathrm{macro}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FN_{i}}$$

where $TP_i$ is true positive given a category $i$. Basically, micro-averaging gives equal weights to every document, while macro-averaging gives equal weights to every category\todo{opět citace \url{http://datamin.ubbcluj.ro/wiki/index.php/Evaluation_methods_in_text_categorization}}.

\section{Results}
All the results are rounded to two significant digits.
\subsection{Logistic regression}
\subsubsection{Random baseline}

For comparison, we began with two ``baseline'' algorithms - first was random classifier, that didn't look at the features at all; the second was logistic regression model, but learned on randomly generated 200 columns of random ``features''.

% v resultech - první micro, potom macro
\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & \textbf{Micro precision} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
Random classifier & 0.11 & 0.51 & 0.19 & 0.11 & 0.57 & 0.18 \\ \hline
Random features & 0.30 & 0.36 & 0.32 & 0.10 & 0.14 & 0.12 \\ \hline
\end{tabular}

\subsubsection{Morphological features}
When we take the features as described in the section \todo{reference}, we got these results:


\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & \textbf{Micro precision} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
Morphology & 0.57 & 0.66 & 0.61 & 0.46 & 0.41 & 0.43 \\ \hline
\end{tabular}

If we break it into individual classifiers, we got these results


\begin{tabular}{|r|r|}
    A & 0.57 \\ \hline
    C & 0.75 \\ \hline
    R & 0.66 \\ \hline
    K & 0.58 \\ \hline
    V & 0.78 \\ \hline
    H & 0.57 \\ \hline
    Z & 0.11 \\ \hline
    M & 0.18 \\ \hline
    P & 0 \\ \hline
    N & 0 \\ \hline
    F & 0 \\ \hline
    D & 0.40 \\ \hline

\end{tabular}

You can see that we got a 0 F-score with several categories. This unfortunately happens, because there is very little examples for those categories in the \emph{testing} data and they all got misaligned.

\subsubsection{Context}
If we take as a context feature 1 word on the left and 1 word on the right, and $min$ set as 40, we have 980 features. With those, results are as follows:


\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & \textbf{Micro precision} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
Context with distance 1 & 0.27 & 0.57 & 0.36 & 0.23 & 0.48 & 0.30 \\ \hline
\end{tabular}


If we take words with distance 2 (with $min$ still 40), we got 1,501 features. The results are as follows:

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & \textbf{Micro precision} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
Context with distance 2 & 0.28 & 0.62 & 0.39 & 0.23 & 0.49 & 0.32 \\ \hline
\end{tabular}

We can see that the results are not that much better.

\subsubsection{Combination - endings and beginnigs of context words}
What we can do now is add the ``combination'' of morphology and context - that is, 
we can try to add endings and beginnings of the words on the left and on the right.

We originally wanted to let the $min$ value 40 to keep the experiments ``consistent'',
but the feature space exploded quickly. So, as a way of a very primitive feature cutting,
we increased the $min$ value for the features where we take more letters.

If we take the endings and beginnings of context words with the distances 1, we have these results:

(We took $min$ as 40 for the lengths 1, 80 for the length 2 and 140 for the length 3. 
Those numbers are quite arbitrarily chosen to keep the feature space relatively small, 
so the algorithms don't become too slow and the feature space too big.)

%\todo{najít ??}
\begin{table}
\begin{tabular}{|r|r|r|r|r|r|r|r|}
     & \textbf{Feature count} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
1 letter & 138 & 0.22 & 0.47 & 0.30 & 0.20 & 0.43 & 0.29 \\ \hline
1, 2 letters & 917 & 0.25 & 0.54 & 0.35 & 0.22 & 0.48 & 0.30 \\ \hline
1, 2, 3 letters & ?1700? & 0.26 & 0.54 & 0.35 & 0.23 & 0.48 & 0.31 \\ \hline

\end{tabular}
\caption{Combination context and endings}
\end{table}
As you can see, with the added 3rd letter, we are not getting much better and 
the feature space is growing, making our algorithm slower. We are also still worse than with the context alone.

%\todo{Tohle je lež, nechtělo se mi to dobíhat do konce}We had similar results with the experiments with the context 
%of distance two.

We can wonder if the reason, why the context is not working as 
well as we would want, is that the feature space is simply too big. 
However, even with reducted space, we never got better results from this `
`combined'' way than from pure context; that means, that all we can get out of context 
is best done by looking at the context alone, not at its endings or beginnings.

\subsubsection{Combination - putting features together}
We thought that with combined feature from context and morphology, the approaches would complement 
themselves and the results would get better, than either of them (especially morphology).

However, this didn't turn out to be true - the results are very slightly worse, than with morphology alone, 
further underlining the fact that the morphology of the words, at least in our case and with our technique, 
is more useful than context.

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & \textbf{Micro precision} &  \textbf{Micro recall}  &  \textbf{Micro F-Score} & \textbf{Macro precision} &  \textbf{Macro recall}  &  \textbf{Macro F-Score} \\ \hline
Combined features & 0.54 & 0.70 & 0.61 & 0.33 & 0.42 & 0.37 \\ \hline
\end{tabular}

%\todo{???}I don't know what more to write but I won't add any new tables. Blah blah blah.

\subsection{Suffixes and a semantic category}
%It is a known fact, that
%In (Kirchner?) it was stated, that 
One of the linguistically valuable side-effects of our research can be 
the set of endings of words that are most likely to belong to the
respective semantic category.
The form of a word can to some extent signalize the semantic class of it. 
For example, 'er' or 'ee' denote [+human, profession] semantic category of a word.
Below are the examples of the endings that
can indicate the semantic features of words according to our experiments.
%Because of the experiment setup it is rather tricky to trust high precision
%score. 

%\begin{table}
%\begin{tabular}{ll}
%\hline
%A abstract & \textit{ivost, ekce, ování, íra, ita, nictví}\\
%H animated & \textit{}\\
%C activity &&& &&&\\
%D action  &&& &&&\\
%R result &&& &&&\\
%K concrete &&& &&&\\
%M measure/unit &&& &&&\\
%N instrument &&& &&&\\
%V property &&& &&&\\
%INT interval &&& &&&\\
%INS institution &&& &&&\\
%P program &&& &&&\\
%F function &&& &&&\\
%Z machinery &&& &&&\\
%\end{tabular}
%\caption{Sem.categories and the endings they take most frequently}
%\end{table}

%Here we faced a problem of choosing between precision and recall.
%For instance, the suffix 'ík' in Czech 

\section{Conclusion}
In this paper we described machine learning algorithms
created to guess the semantic category of a noun based on the following features:\\
1.Context surroundings of a word\\
2.Morphological characteristics of a word\\
We have shown, that for this concrete task the context properties of a word
does not appear to be promising at least in such a simple manner we
used(regarding context as a word after and a word before) .
Probably, more non-trivial context features should be taken into account
- such as ex. part-of-speech tag.
On the other hand, such simple method as guessing semantic features on the
basis of the surface form of the word brought more favorable %positive?promising?
results in terms of precision. It was also an interesting observation, that
adding context features to that morphological to the machine learning
negatively influenced the overall quality.

The possible future directions of our work will lie both in the field of
optimizing features for the Machine Learning algorithm as well as
adapting WordNet as training and test data for our task.

\bibliographystyle{aaai} \bibliography{biblio}


\end{document}
