%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
%\usepackage{todonotes}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
%\newcommand{\todofnfootnote}{}
\newcommand{\todofn}[1] {
 \footnote{\textbf{TODO : #1}}}

\usepackage{url}
\frenchspacing
\pdfinfo{
/Title (Exploiting Maching Learning for Automatic Semantic Feature Assignment) %
%/Subject (AAAI Publications)
/Author (Karel Bilek, Natalia Klyueva, Vladislav Kubon)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%

\title{Exploiting Maching Learning for Automatic Semantic Feature Assignment}
\author{Karel B\'{i}lek, Natalia Klyueva, Vladislav Kubo\v{n}\\
Charles University in Prague\\
Czech Republic}
\maketitle
\begin{abstract}
In this paper we experiment with supervised machine learning techniques for
the task of assigning semantic categories to nouns in Czech. The experiments work with 16 semantic categories 
based on available manually annotated data. 
%relevant especially for the task of Word Sense Disambiguation. The training
%data presents a set of 2,783 nouns with assigned semantic categories.   
The paper compares two possible approaches - one based on the contextual information, 
the other based upon morphological properties  - we are trying to automatically extract final segments 
of lemmas which might carry semantic information. The central problem of this research is finding the features for machine learning 
that produce better results for relatively small training data size. 
%We exploited the context of a word which is a generally used technique for this task, 
%and we also tried the morphological segments as features.

\end{abstract}


\section{Introduction}

Lexicons enhanced with semantic information are frequently used 
in various NLP applications, such as machine translation, question-answering
or sentiment analysis. Probably the most well known resource of such kind is 
WordNet \cite{wordnet}, the lexicon of words
interlinked by semantic relations and organized hierarchically into 
semantic classes. It nowadays exists for many languages including Czech. Although it is a large scale resource providing complex semantic information, it's applicability is often limited by the fact that it was created by means of a translation of the English WordNet and that it uses a system of categories which may not fit a particular application.

Many additional tools for automatic semantic annotation have been created so far, as, e.g., 
for semantic relation assignment \cite{peirsman} or multipurpose semantic memory \cite{baroni:2009} etc.  
 
In this paper we describe experiments with automatic assignment of semantic features (categories) to Czech nouns
exploiting an existing resource (a small hand-annotated lexicon created originally for a machine translation system). The assignment is performed by means of logistic regression models. The model is trained in a supervised manner, using 
%using a small training set of nouns that are annotated with semantic categories. 
%We are excperimenting with 
basically two kinds of features for machine learning --  morphological and syntactic (context behaviour) properties and their combination. The experiments suggest an answer to a question which type of features is more useful for the given purpose.   


\subsection{Note on Notation}
Since the word ``feature'' is frequently used both in linguistic context as a term for semantic category and in 
machine learning context as a term for describing any observable property of a learning example, for 
better clarity, we use the term ``category'' for the notion of semantic feature, 
while we use the word ``feature'' purely for machine learning feature in the rest of this paper.


\section{Motivation}
\subsection{Semantic Categories}
Semantic categories are generally viewed as components of meaning
that express one definite sense of a word, they are generally associated
with the contrastive context, so they occur either with plus or minus
sign, ex. [+human], [-time] etc. 
There is no generally accepted set of semantic features, as researches
usually use their own classification depending on their goals. Assigning 
semantic features to concrete words is influenced by a general issue 
of finding the proper level of granularity - if the number of semantic features is relatively 
low, they are not rich enough to capture more subtle differences in the woird meaning;
if too many of them are used, it is more difficult to assign them correctly
for each particular word. This is probably one of the reasons why the 
number of distinct semantic features varies in various experiments 
from only a few (like
the division of nouns into 'animated' or 'non-animated') to the very
fine-grained meaning classification (as, e.g., in WordNet).

Semantic categories can also be used for some minor research problems, as, for example
resolving nominative-accusative ambiguity in Slavic languages.
In \cite{principled_disam} the authors show the way to disambiguate adjectives
with the help of semantic categories of nouns they modify. 

For example, it may help 
to discriminate two senses of the adjective \textit{short}
: applied to those nouns with the sem. category [+human] versus in combination
with a [+interval] noun. The phrase \textit{a short girl} is translated
into Czech as \textit{mal\'{a} holka}, whereas \textit{a short day} is \textit{kr\'{a}tk\'{y} den}.

It can also help to disambiguate different senses of verbs:\\ 
(1)\textit{The dog runs after the owner.} - [+human] category of a subject\\
(2) \textit{The program runs on Linux.} -[+computer] category of a subject \\
If translated for example from English or Czech into Russian, the disambiguation is needed:
while the verb in (1) can be translated straightforwardly as \textit{be\v{z}a\v{t}} - 'to run' into Russian,
the metaphorical meaning in (2) must be expressed by another verb -- \textit{rabota\v{t}} - 'to work'.

\subsection{Classification}
The task of semantic category assignment can be naturally represented as a task of classification, using supervised machine learning algorithms.

Generally, in supervised machine learning, we have a set of training data with defined 
categories and we want to build an algorithm, which will generalize from this training data and return a category for any yet unseen data.

More specifically, in most of the algorithms, the training data can be broken into
\textit{examples}, and each of the examples can be described as a set of \textit{features} and a given \textit{category}. The machine learning algorithm is usually a model with variable parameters, which are then learned from the training data.

With the data we have at our disposal, we can take the semantically annotated words from the lexicon as training examples 
and their linguistic properties as features in machine learning.


\section{Sources}
\subsection{Semantic Categories}
Reliable training data constitute the necessary condition for the success of any supervised machine learning algorithm. Instead of creating a new set of semantic categories and subsequently undertaking a long and costly process of manual annotation, we have decided to re-use existing high quality manually annotated data. Such data exist in the form of a bilingual dictionary of a machine translation system RUSLAN, a rule based MT system translating from Czech to Russian.

The history of RUSLAN \cite{oliva1989parser} goes back to the second half of the 80's when there was a need for an automatic translation of operating systems manuals. However, due to the political changes after 1989, 
there was no need for such MT between Czech and Russian anymore. Since then, the resources used in the project served mainly as a source of data for other projects, for
example, in \cite{mt-recycled} authors tried to re-use the module
of syntactic analysis of Czech for the Czech-English Machine Translation,
the paper \cite{pisa2010} describes the extraction of morphosyntactic information from the bilingual dictionary of RUSLAN for Czech and Russian valency dictionaries.

The bilingual dictionary of RUSLAN constitutes a rich source of various kinds of data for both Czech and Russian. Apart from the target language equivalents and their morphology, the dictionary provides  morphological, syntactic and semantic information for the source language (Czech), namely declension patterns, valency frames, semantic features etc. In the current experiment we ignore the Russian (target language)
part of the dictionary and we use (a relevant part of) the Czech side, namely the semantic features assigned to all nouns in the dictionary. 

Although the high quality of human annotation of the words in the dictionary is a valuable asset, the dictionary of Ruslan also has one drawback - a relatively limited domain. The project aimed at the translation of manuals to operating systems of mainframes. It thus contains a relatively smaller number of domain-independent words, the domain related expressions prevail. On top of that, even the computer terminology has changed during the past 25 years, so  some of the words contained in the dictionary are slightly outdated. 

\subsubsection{Reusing Data from RUSLAN Dictionary}

The dictionary of RUSLAN contains about 8,000 entries, 2,783 of which are nouns with assigned semantic categories.

Following is an example of an entry:

\begin{verbatim}
LE2KAR3==MZ(@(*H),!,MA0111,VRAC2).
\end{verbatim}

\begin{itemize}
\item \texttt{LE2KAR3} represents the Czech lemma \emph{l\'{e}ka\v{r}}; the diacritics is encoded in a rather primitive way corresponding to the time when it was created and when encoding of national characters still constituted a challenge.
\item \texttt{MZ} represents a declension pattern (and thus also determines the part of speech information because this particular declension pattern is used for masculine animate nouns in Czech).
\item \texttt{@(*H)} represents the semantic category `animated'.
\item \texttt{MA0111,VRAC2} represents the declension class of the Russian equivalent and the equivalent itself, encoded into basic ASCII
\end{itemize}

From this format we have extracted the Czech side of the dictionary together with the semantic categories. 
When we took the categories without ``sanity checking'' and filtering out the possible mistakes, 
we ended up with 2,783 words and 29 categories; however, some of these categories appear only with one or two words, 
therefore they are not relevant for our purpose. When we filter out those categories, that don't appear in at least 30 words, we ended up with the features described in Table 1.

\begin{table}
\begin{tabular}{|l|l|l|}
 \hline
\textbf{Shortcut} &  \textbf{Count} & \textbf{Category}\\
\hline
A & 941 & abstract \\ \hline
C & 835 & activity \\ \hline
R & 728 & result \\ \hline
K & 712 & concrete \\ \hline
V & 205 & property \\ \hline
H & 165 & animated \\ \hline
Z & 101 & machinery \\ \hline
M & 64 & measure\\ \hline
P & 56 & program \\ \hline
N & 44 & instrument \\ \hline
F & 41 & function \\ \hline
D & 32 & action \\ \hline

\end{tabular}
\caption{Semantic categories in training data}
\end{table}


The total number of categories assigned to words is bigger than the number of words because some words have more than one semantic category. In average, each words is assigned 1.4 categories.



%\subsubsection{Semantic features - negative experience}
%Before we'll introduce additional sources of data, let us make a short remark concerning the role of semantic features in the original system. The main reason why they were introduced was the endeavor to reduce the ambiguity of translation (the system tended to slightly overgenerate the results) and to disallow sentences which would not be semantically plausible. The practical experience from testing clearly showed that using the features for restriction of certain is not a good idea, they tended to be too restrictive and actually blocked the translation more often than they helped to resolve the ambiguity.


\subsection{Monolingual Data Corpus}
Because one of our methods was supposed to exploit an immediate context, it was necessary to use additional annotated corpora. As the highest quality annotation of Czech is provided in the Prague Dependency Treebank (PDT),\footnote{\url{http://ufal.mff.cuni.cz/pdt2.0/}}, it was a natural first choice.   
% data sourcea context, we As one of our features, we decided to use the context informations, taken from a monolingual corpus.


\subsubsection{PDT}
PDT 
%- Prague Dependency Treebank \url{http://ufal.mff.cuni.cz/pdt2.0/} - 
represents a collection of Czech texts annotated on three
levels - morphological, analytical(surface syntactic) and tectogrammatical(deep syntactic). It   
contains 115,844 sentences from newspapers and journals.

However, we have found out that even the size of PDT does not provide a sufficient coverage of the words from the RUSLAN domain. 
%We expected most the words to appear at least once in 
The 115,844 sentences of PDT contain 1,957,247 tokens, but out of the 2,783 words from RUSLAN, 813 don't appear at all in the entire corpus, 162 appear only once and 
1,408 words appear less then 10 times.

These numbers clearly indicate that the contextual information based on manually annotated data from RUSLAN is too sparse for machine learning. In would actually mean that many feature vectors would be empty 
%for about half of the examples in the test 
%set,
which would cause 
%half of the 
many examples to be misaligned completely.

For that reason, we decided to use a bigger corpus with only morphological annotation.

\subsubsection{WebColl}

WebColl\cite{webcoll} is a corpus of texts in Czech crawled from the Czech web, cleaned and annotated  with a POS tagger and lemmatized.
WebColl consists of 7,148,630 sentences, which together have 114,867,064 tokens. 

Although this corpus is about 100 times bigger than PDT, its data cover our lexicon only slightly more. Out of the 2,783 words, 412 don't appear at all, 
40 appear exactly once and 611 words appear less than 10 times. In other words -- increasing the training data size approximately 100 - times results in the removal of only about a half of the unseen words.

The manual revision of the unseen words revealed that most of those words are very domain specific 
(words such as ``\emph{rebasing}'', ``\emph{subroutine}'', ``\emph{self-relocability}'' and so on) 
and that they probably won't appear frequently enough no matter how big corpus we take. With regard to the rest, some of those words were genuine mistakes and some of them were affected by slightly different lemmatization used in RUSLAN and WebColl (incompatible lemmas).


\section{Machine Learning Features}
The success of machine learning algorithms to great extent depends on the 
%One of the major decisions for us was 
choice of proper 
features.
% for the machine learning
In our experiments we have tried to exploit two types of features 
%The features we used in our work are
%of two types 
- context ones and morphological ones. 
In other words, we exploited syntax and morphology in order to learn semantics of a word.  

\subsection{Context}
As J. R. Firth stated ``Words shall be known by the company they keep'',
therefore our first idea (and the main reason for using a large mono-lingual corpus) 
was to look at the context in which the words appear and to try to convert it into machine learning features.

The context can in principle be exploited in a number of different ways. For example,
in \cite{baroni:2009}, the authors proposed a scheme to retrieve
various semantic properties of words from the context.
%use a method of automatic clustering,
%where semantic relations have been retrieved from context.
In \cite{biemann05} the semantic features of nouns were learned
from a context, but only adjectives were taken into account.
We have decided to use all parts of speech as a context in our experiment. This decision was motivated by the endeavor to use as much information from the context as possible. 

We have taken into account the context of two words to the left and two words to the right. 
First of all, it was necessary to determine the context of all ``known'' words from the RUSLAN dictionary, i.e. the words whose semantic categories have been assigned manually and thus can be considered reliable and correct.  

For every word from the dictionary, we 
have looked at all words in its context.
If $n$ types were found, then it actually meant that we have obtained $n$ separate 
features for that particular word, where the value of each feature represents 
the number of times when the feature word appeared in the context.
 
The machine learning model was then trained on these features. To assign a category
to an unseen word, we had to go through the entire corpus, count
the features for the unseen word as well using the same algorithm as for the features of the known words. However, this time we were not collecting all words appearing in the left or right context of the unseen word, but only those appearing among the features collected for known words (in other words, we counted all the words in the context and then performed an 
intersection of the counted words and the words from the features). Again, the number obtained for each feature represented the value of that feature for the unseen word and became a part of the feature vector of the machine learning model.

This naive approach had several drawbacks. Most importantly, our number of features exploded, while the values (number of appearances) themselves were very unevenly distributed.

For this reason we made several adjustments:
\begin{itemize}
    \item we nominated a context word as a feature only when the given context word was seen in at least some fixed number of training examples $min$. The motivation for this decision was very simple - it a given word appears in the context only few times, it does not tell much about the context (a ``training example'' here means a word from the dictionary)
    
    \item we normalized the numbers, so that the features were all approximately of 
the same size. We originally wanted to use percents as values - meaning, 
we wouldn't have the number of appearances as a feature, but the percentage of how often is a given word in the context of the training example.    
     However, this resulted in very small numbers, since actually, 
in most of the training examples, even the top context features are in the order of 
fractions of 1 percent; so, at the end we have decided to use percentage multiplied by ten and converted to integers. 
We ignore the values when the integer value is smaller than 1 (that means, if it is the context in less than 1/1000 of cases).
     

\end{itemize}

If we set $min$ as 40, the number of features is 1,502.

\subsection{Morphology}
The second approach we have decided to try was based upon morpho-semantic properties of nouns.
The main idea of this experiment was based upon the investigation carried out by Z.Kirschner in his system MOSAIC \cite{kirschner1983}. He exploited the fact that many suffixes in natural languages determine the semantic nature of words. For example, in English, the suffixes {\it -or} or {\it -er} usually appear with words having a semantic role of an actor of some activity, {\it -tion} is an activity, {\it -ity} or {\it -ness} marks a property. In Czech,  {\it -i\v{c}}, {\it -a\v{c}}, {\it -\v{c}ka}, {\it -\'{e}r}, {\it -or}, {\it -dlo}, {\it -metr}, {\it -graf}, {\it -fon}, {\it -skop} are tools or machines, {\it -ace}, {\it -kce}, {\it -\'{a}\v{z}}, {\it -n\'{i}}, {\it -za} processes or activities,
{\it -ost}, {\it -ita}, {\it -nce} properties; and adjectives ending with {\it -an\'{y}}, {\it -en\'{y}} are results of processes while {\it -ac\'{i}}, {\it -ec\'{i}} marks a purpose.

There were two major reasons why we could not apply the method from MOSAIC directly. First, the semantic categories determined by Czech suffixes do not directly correspond to the set of semantic categories we are using in our experiments; and, second, the number of suffixes seems to be too large (\cite{kirschner1983} says that a full coverage of Czech technical texts would require about 2000 suffixes) and contain too many exceptions (for each domain it is necessary to create a dictionary of hand-picked exceptions, words, which have the particular suffix, but which do not belong to a semantic category usually marked by the suffix).     
  
In order to avoid the long and costly process of manual selection of relevant suffixes, we have decided to rely on data and  
%We then tried to experiment with a morphology, altough we initially saw it as just a ``baseline'' model.
%Because in Czech language, the ending can often determine the semantic category, 
to try adding suffixes as a machine learning feature.  
More precisely, we took the last $n$ letters from 
a word (for $n=$ 4,3,2 and even 1) and we then created a new feature for every such suffix. 
%with a valand set is as 1.

It actually resulted in the feature vector for any training example consisting mostly of zeroes 
(it has 1 only for one ending of length 4, one ending of length 3, etc., all the other features being equal to 0). 
On the other hand, the advantage of such brute-force approach is obvious - 
it is much easier to find such feature 
for a new word, because it is fully determined by the word itself and it is not necessary to use any other source of information.

Also the morphological features tend to explode quickly, therefore it was necessary to apply the same measures to stop feature explosion - we used only those suffixes that appeared at least $min$-times in the dictionary.

With $min$ set to 5, the number of features reached a reasonable value of 433.

\subsection{Suffixes and Semantic Categories}
%It is a known fact, that
%In (Kirchner?) it was stated, that 
The identification of relevant suffixes is not sufficient,
as a second step it is also necessary to link the suffixes with relevant semantic categories. 
%One of the linguistically valuable side-effects of our research can be 
%the set of endings of words that are most likely to belong to the
%respective semantic category.
%The form of a word can to some extent signalize the semantic class of it. 
%For example, 'er' or 'ee' denote [+human, profession] semantic category of a word.
Below are some examples of endings that
can indicate the semantic features of words according to our experiments.

\begin{table}[!h]
\begin{tabular}{ll}
\hline
A abstract & \textit{ivost, ekce, ování, íra, ita, nictví}\\
H animated & \textit{átor, ovník, atel, atelka, stník, or, ík}\\
C activity & \textit{ení, ování, ání, ace}\\
D action  & \textit{ení, ání, ace}\\
R result & \textit{ ita, ení, utí, ání, akce, utí}\\
K concrete & \textit{ka, íč, énko, ora}\\
M measure/unit & \textit{etnost, ita, ví, o, kost, etí}\\
V property & \textit{ita, nost, ce}\\
\hline
\end{tabular}
\caption{Sem.categories and the endings they take most frequently}
\end{table}




\section{Machine Learning Appropaches}
\subsection{Logistic Regression}
The most suitable tool for our experiments seems to be the logistic regression. It can predict to which category a particular unseen word belongs. If we have several binary classifiers, it is recommended to use \emph{one-vs-all} models, with the category with the biggest chance ``winning''. However, this model is only applicable to the case where we have \textbf{\emph{single category}} with every training case. 
In our case, though, we have multiple categories and their assignment is therefore not so straightforward.
%(also called multiple labels in literature). 

In order to solve this problem, we have employed a simple solution, where every category has its own classifier trained separately, returning 1 or 0 and thus indicating whether a particular input word belongs to this category or not.
%, indicated 
%belonging or not belonhing to the category. 
The item is then indicated as belonging to all the categories where the classifier returned 1.

For every experiment, we put aside the same (at the beginning randomly selected) set for testing purposes. 
On the training set, we put aside a heldout set, and we train the parameters $\lambda$ and $\gamma$ on the heldout set, simply by training the model with the given $\lambda$ and $\gamma$, counting the F-score and getting the best $\lambda$ and $\gamma$ for the given classifier.

These parameters vary from classifier to classifier; for a given feature set, we train 14 classifiers (one for every semantic category), together with training $\lambda$ and $\gamma$ parameters, and then we test those categories on the test set.

\section{Machine Learning Evaluation}
Because we use multi-labeled classification, we cannot use only precision and recall. Instead, we use so-called micro-average and macro-average of both precision and recall, and then make their mean average for micro-averaged and macro-averaged F-score. We use $\mu$ as a symbol for micro and $M$ as a symbol for macro.

Micro- and macro-averaged precision and recall is defined as \footnote{From Data Mining wiki  \url{http://datamin.ubbcluj.ro/wiki/index.php/Evaluation_methods_in_text_categorization}, Babes-Bolyai University, Romania} 

$$P_{\mathrm{\mu}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FP_{i}}; \quad R_{\mathrm{\mu}} = \frac{\sum_{i=1}^{|C|}TP_{i}}{\sum_{i=1}^{|C|}TP_{i}+FN_{i}}$$

$$P_{\mathrm{M}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FP_{i}};\quad R_{\mathrm{M}}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_{i}}{TP_{i}+FN_{i}}$$

where $TP_i$ is true positive given a category $i$ and $C$ is the set of categories. Basically, micro-averaging gives equal weights to every document, while macro-averaging gives equal weights to every category.

\section{Results}
First of all, let us present the results achieved for the baseline experiment and for both experiments using the above described sets of features. 
All results are rounded to two significant digits.

\subsection{Logistic Regression}
\subsubsection{Random Baseline}

For comparison, we began with two ``baseline'' algorithms - first it was a random classifier, that didn't look at the features at all; the second was logistic regression model, but trained on randomly generated 200 columns of random ``features''.

% v resultech - první micro, potom macro
\begin{table}[h]
\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Rand. class. & 0.11 & 0.51 & 0.19 & 0.11 & 0.57 & 0.18 \\ \hline
Rand. feats. & 0.30 & 0.36 & 0.32 & 0.10 & 0.14 & 0.12 \\ \hline
\end{tabular}
\end{table}


\subsubsection{Context}
In order to find out whether the context based method provides enough information for semantic classification, we have not tested all possibilities (various length of the context, different threshold value $min$ etc.). In the first experiment we have taken only 1 word on the left and 1 word on the right as a context feature, and we have set $min$ to 40. This gave us 980 features. With those, the results are as follows:

\begin{table}[h]

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Dist. 1 & 0.27 & 0.57 & 0.36 & 0.23 & 0.48 & 0.30 \\ \hline
\end{tabular}
\end{table}

The second experiment extended the context to words with distance 2 both to the left and to the right 
(with $min$ still 40). This increased the number of features only slightly, to 1,501. The results were unfortunately not much better, thus indicating that further extension of the context is most probably irrelevant:

\begin{table}[h]
\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Dist. 2 & 0.28 & 0.62 & 0.39 & 0.23 & 0.49 & 0.32 \\ \hline
\end{tabular}
\end{table}



\subsubsection{Morphological Features}
As the next step, we have tested the logistic regression with morphological features obtained in the way described above. The results are quite encouraging compared to the experiments with the context, they constitute a substantial improvement:

\begin{table}[h]
\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Morphology & 0.57 & 0.66 & 0.61 & 0.46 & 0.41 & 0.43 \\ \hline
\end{tabular}

\end{table}

If we break the results into individual classifiers, we can see that for several categories it is quite successful with relatively high F-score, thus indicating that certain suffixes related to some categories are really quite productive and that the assumption that they carry the meaning of the word is probably right. On the other hand, 0 F-score with several categories represents a complete failure of the mechanism. It happened due to a very small number of examples for those categories in the \emph{testing} data, and due to the fact that they all got misaligned. This result might also indicate that the set of semantic categories was itself only partially well-chosen. Some categories are too seldom in order to be taken into account, certain redefinition might be useful for future experiments. The exact results are presented below: 


%\begin{table}[h]
\begin{tabular}{|r|r|}
\hline
abstract & 0.57 \\ \hline
activity & 0.75 \\ \hline
result & 0.66 \\ \hline
concrete & 0.58 \\ \hline
property & 0.78 \\ \hline
animated & 0.57 \\ \hline
machinery & 0.11 \\ \hline
measure & 0.18 \\ \hline
program & 0 \\ \hline
instrument & 0 \\ \hline
function & 0 \\ \hline
action & 0.40 \\ \hline 

\end{tabular}
%\end{table}


\subsection{Combination of Approaches}
The encouraging results of the morphological classifier led to the third experiment. The main question of this experiment was whether the context can add some information to the suffixes.
We have tested two possible ways how to combine context and morphology. The first one was a bit more sophisticated, we tried to include the morphology of the context. 
That means - we tried to add features as ``the sum of the combinations of last 3 letters of the words with the distance max. 2 to the right'', and so on.

We originally wanted to let the $min$ value 40 to keep the experiments consistent,
but the feature space exploded quickly. So, as a way of a very primitive feature cutting,
we increased the $min$ value for those features where we take more letters into account.

The endings and beginnings of context words with the distances 1 gave us these results:
(We took $min$ as 40 for the lengths 1, 80 for the length 2 and 140 for the length 3. 
Those numbers were quite arbitrarily chosen to keep the feature space relatively small in order to prevent the algorithm becoming too slow and the feature space too big.)


\begin{table}[h]


\begin{tabular}{|r|r|r|r|r|r|r|r|}
\hline
letters&feats & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
1 & 138 & 0.22 & 0.47 & 0.30 & 0.20 & 0.43 & 0.29 \\ \hline
1, 2  & 901 & 0.25 & 0.54 & 0.35 & 0.22 & 0.48 & 0.30 \\ \hline
1, 2, 3  & 1659 & 0.26 & 0.54 & 0.35 & 0.23 & 0.48 & 0.31 \\ \hline

\end{tabular}
%\caption{Combination context and endings}
\end{table}

With the added 3rd letter, the F-scores are almost the same as for less complicated combinations while the feature space has grown substantially, making our algorithm slower. The worst aspect is, however, that the results are actually even worse than the results for the context alone.

We can only wonder whether the reason why the context is not working as expected, is that the feature space becomes unrealistically big and the so-called Curse of Dimensionality starts to take place.\footnote{This is sometimes known as ``Hughes effect'', see \cite{hughes:1968}} Basically, we get a huge matrix with more features than training examples, but the matrix is very sparse.
 
However, even after a substantial reduction of the space in subsequent experiments, we were never able to get better results from this `
`combined'' approach than from pure context. This means that all we can get out of context 
is best done by looking at the context alone, not at its endings or beginnings.




\subsubsection{Combination - Putting Features Together}
The second type of combination is actually more primitive than the first one. It simply takes both types of features and puts them together. 
However, neither this simple combination provided better results than morphology alone, 
further underlining the fact that the morphology of the words
seems to be more useful than context.

\begin{tabular}{|r|r|r|r|r|r|r|}
 \hline
 & $P_\mu$ & $R_\mu$ & $F_\mu$ & $P_M$ & $R_M$  & $F_M$ \\ \hline
Combined & 0.54 & 0.70 & 0.61 & 0.33 & 0.42 & 0.37 \\ \hline
\end{tabular}



\section{Conclusion}
In this paper we have described the experiments in applying machine learning algorithms
to guessing the semantic category of nouns based on the following features:\\
1. A (limited) context surrounding a word  \\
2. Morphological characteristics of a word, namely its suffix\\

We have shown that context properties of words
provide less information than suffixes, both alone and in combination. 
Even a very simple method of guessing semantic features on the
basis of the surface form of a word brought more favorable %positive?promising?
results in terms of precision. It was also an interesting observation, that
adding context features to the morphological ones in machine learning
negatively influenced the results.

There are several possible directions for further research. One lies in the field of
optimization of features for Machine Learning algorithm, the second one might explore 
adapting WordNet as training and testing data for our task. The third one points towards
designing non-trivial context features taking into account part-of-speech tags and other information.
Last but not least is the effort to obtain a bigger number of hand-annotated data of a more general nature than 
the rather domain specific data we had at our disposal in these experiments. 
 

\section*{Acknowledgments}

The research was supported by the grants GACR P406/2010/0875 and GAUK 639012.

\bibliographystyle{aaai} \bibliography{Biblio}


\end{document}
