\chapter{Translation systems}
\label{chapter:systems}
Machine translation (MT) is a task that's as old as the computer. When the very first computers were created for the task of encryption and decryption, one of the other areas of interest was translation of natural languages.\footnote{As noted in the introduction in \cite{koehn2010statistical} -- \uv{The history of machine translation goes back over 60 years, almost
immediately after the first computers had been used to break encryption codes in the war, which seemed an apt metaphor for translation: what is
a foreign language but encrypted English?}}

Machine translation between Czech and Russian has, too, some history.

In this chapter, I am describing both historical and more recent approaches for machine translation between those two languages. In the chapter~\ref{chapter:experiments}, I am then describing our experiments with those systems.

\section{Statistical vs. rule-based -- an overview}
\label{axis}
MT systems has historically used many different approaches. One way of classifying them is on the axis of rule-based vs. statistical.

In general, we can re-use the definition, used in \cite{bojar}, which is as follows\footnote{The following list is a rough translation from the mentioned book}:
\begin{itemize}
\item rule-based MT systems:
\begin{itemize}
\item use analysis, transfer and synthesis steps
\item use formal grammars
\item use hand-made dictionaries
\item have linguistic information hard-coded and therefore aren't lan\-guage-ag\-nos\-tic
\end{itemize}
\item statistical MT systems
\begin{itemize}
    \item use more variants of outputs, rank them with some score, and choose the best one
    \item train internal dictionaries from big parallel data
    \item have more compact translation core, their inner working are less obvious
    \item use statistics instead of linguistic rules and therefore are more language-agnostic
\end{itemize}
\end{itemize}

However, with actual, real-life systems, the distinction is usually not as clear-cut. 
For example, statistical MT systems like Moses (\ref{systems:moses}) can get significantly better results with added linguistic information; 
on the other hand, systems like TectoMT (\ref{systems:tecto}), which can for some intents and purposes be classified as more rule-based, have individual parts in some way based on statistics.

\section{Rule-based systems}

%The whole reason for focusing on Czech-to-Russian translation in this thesis, as opposed to the opposite direction, was the history of Cze.

%Unfortunately, I haven't been able to get any of the historical syst

%\section{Unrunnable systems}
%In this section I will present some historical systems, that I haven't been able to get successfully running.

\subsection{RUSLAN}
\label{systems:ruslan}
RUSLAN is a machine-translation system, developed between 1984 and 1988 at several departments of Charles University, Prague. RUSLAN firmly belongs to the \emph{rule-based} category, since at that timeframe, statistical machine translation wasn't even invented yet.




%However, \cite{nguyen2009systemes} describes (unsurprisingly, also in French.) modern-day experiments with Systems Q, also mentioning, that all Fortran implementations has been lost\footnote{Which would make UFAL's version, if it was working, quite unique.} and that he reimplemented it in C. I haven't been able to try this version.



Description of the system can be found in \cite{olivaruslan} or \cite{hajic1987} -- however, the reader has to bear in mind that both the systems \emph{and} their manuals and descriptions are severly dated. (At least for me personally, especially the book \cite{olivaruslan} was hard to read and navigate in.) Contemporary (but not as detailed) description of the system can be found in \cite{recycled}.

The whole RUSLAN system has several components:
\begin{pitemize}
\item preprocessing, written in Pascal
\item morphological analysis, using dictionary, written in Q-Systems (described further) and interpreted in Fortran IV
\item syntactico-semantical analysis, using morphology, also written in Q-Systems; this component uses FGD as its theoretical starting point
\item generation, also using Q-Systems
\item morphological synthesis, using Pascal
\end{pitemize}

\subsubsection{Q-Systems}
Q-Systems (sometimes also Systems Q) -- Q stands for \uv{Quebec} -- are a tool for machine translation, developed at Montreal University by Alain Colmerauer, also the creator of Prolog.\footnote{See \cite{qsystems}.} 

Q-Systems are similarly declarative as Prolog. This means the author can focus more on the \emph{result} than the \emph{order} of analysis. If there is any ambiguity, all the possibilities are explored \emph{in parallel} (this is how Q-Systems differ from, for example, Prolog). 

In theory, this could make writing lexical rules easier and resulting in simplier rules; in reality, the resulting rules are quite unreadable, as will be seen later.

Q-Systems are not very widely used or widely worked with. One of the reasons might be the fact that all documentation is in French.  


\subsubsection{Dictionary}
RUSLAN uses a Czech-to-Russian dictionary, written by hand in afforementioned Q-Systems. Dictionary item looks like this:

\begin{verbatim}
DLOUH==M(RS(+(*INT)),MI2289,DLINNYJ).
DLOUH==M(RS(-(*INT)),MI2276,DOLGIJ).
\end{verbatim}

This describes two possibilites of the translation of the word \uv{dlouhý} to Russian: the first is \uv{длинный} and the second is \uv{долгий}. They differ by the semantic feature INT they require or forbid from the word they depend on. 

More complex dictionary item looks like this:
\begin{verbatim}
C3ES3TIN
  ==Z(@(*A), MIO109, $(JAZYK), 
     2(POS, #($), &, $(MI28), $(C2ES2KIJ),
       1(=,@($), #($),$($))),
     1(=,@($),#($),$($))).
\end{verbatim}

This item translates the word \uv{čeština} to Russian words \uv{чешский язык} and also describes their relationship.

Maybe because memory was more expensive than today, all dictionary items are used without any comments, leaving only very difficult-to-decypher rules.
\subsubsection{Analysis}

The rules for analysis are even less readable. Random example of two such rules are as follows:

\begin{verbatim}
1(B*, X*1, /, X*2, F*1(C*), X*3, /, X*4, @(V*), X*5, %(X*),
 I(*), 1(X*6, $($)), X*7)

1Z(A*9), (Z*2)
  == 1(D*, /, X*2, F*1/X*),/,@(V*),X*5,%(X*),1*,1(X*6,$($)),X*7,
      A*B,
     5(U*1, @(U*2), U*3, $(U*), 3(E*(Y*1), B*(C*), W*1, W*3, %(X*), 
        $(W*)),
     +1Z(A*9, Z*2)
       / -NON- (, + -DANS- X*9 -ET- +(V*) -HORS- X*9, +(VZT)
        -ET- -(V*) -HORS- X*9, *
        -ET- C* = S
        -ET- X*3,* -HORS- /,N(S), S(S), D(S), A(S), L(S), I(S)
        -ET- / -HORS- X*2, 2
        -ET- (, Y%1 = -NUL-
             -OU- E*(Y*1) -HORS- U*2,*
             -OU- E*(+(V*, *)) -HORS- U*2, *
             -OU- -NON- E*(-(V*)) -HORS- U*2, * .)
        -ET- (. E*(Y*1) *N
             -OU- H(B*(C*)) -DANS- U*1 .) .
\end{verbatim}

Those are all left with next to no comments. For example, the only comment for the group of more than 20 rules, including the two rules above, is \texttt{RELATIVE CLAUSES ADJOINED TO THEIR HEADS}.

\subsection{Česílko 1.0}
\label{cesilko10}

\uv{Česílko} is a name for two entirely different machine translation systems with slightly different goals and, more importantly, slightly different structure. Both were originally intended for Czech-to-Slovak translation.

Česílko 1.0 was a system, developed in 2000, and was aimed for direct translation between Czech and Slovak and intended to assist a translation memory\footnote{See \cite{cesilko1}.}. The translation works lemma-by-lemma in a following fashion:
\begin{pitemize}
\item morphological analysis of source (Czech) language
\item disambiguation
\item direct translation, lemma-by-lemma
\item morphological synthesis
\end{pitemize}



The system is written in a mixture of C, C++ and Flex (fast lexical analyser generator). The code itself is not really well documented and modular, but that can be attributed to the age of some of the components -- despite the whole system being developed in 2000, some files seem to be as old as 1991.

%This system itself is unfortunately not very extendable from Slovak to Russian (as the target language). Partly because of the design itself, partly because translations from Czech to Russians are not doable only word-by-word basis.

%-- as we can see on the examples in the section \ref{sec:experiments} -- the sentences are not really translatable word-by-word.

%For that reason I decided not to further experiment with Česílko 1.0 for Czech-to-Russian machine translation.

\subsection{Česílko 2.0}
\label{cesilko20}
Česílko 2.0 is a different project with similar goals, but using different frameworks and adding more transfer rules\footnote{See \cite{cesilko2}}. 
%However, it has its own shares of problems, that prevented us to use it.

The system works in a following fashion:
\begin{pitemize}
\item \textbf{non-deterministic} morphological analysis of source Czech language
\item translation of lemmas
\item applying transfer rules by changing syntactic tree
\item morphological synthesis
\item ranking of all the generated sentences
\end{pitemize}

Unlike Česílko 1.0, Česílko 2.0 uses a non-deterministic parser and explores all the possi\-bi\-li\-ties in parallel. 

Česílko 2.0 uses more advanced and more clearly defined tranfer rules. This advanced transfer would, in an ideal world, make the system more modular and extensionable for our purposes. 


Česílko 2.0 is written in the language Objective-C. Because Objective-C might not be known to the reader, I will describe it a slightly more detailed manner.

%However, the implementation details prevented us from doing any significant work on Česílko 2.0.
%To illuminate why, let me focus a little on the technical details.

\subsubsection{Objective-C}
Objective-C is a very simple and elegant extension of C language, developed by Brad Cox in 1980s by adding Smalltalk features to C\footnote{See \cite{cocoa4}}. 

Objective-C is, in my opinion, very easy to learn and understand, at least compared to C++, its more popular counterpart.

Objective-C is not a proprietary language and is possible to compile with either gcc or Clang/LLVM compilers. However, what is proprietary is its most used standard library, Cocoa.
I will describe it here, since it will be important in further sections.

\subsubsection{Cocoa}
When Steve Jobs left Apple, he made a smaller company called NeXT. Among other things, they produced a proprietary operating system called NeXTSTEP, based on Unix.\footnote{For a more detailed history, see \url{https://developer.apple.com/legacy/library/documentation/Cocoa/Conceptual/CocoaFundamentals/WhatIsCocoa/WhatIsCocoa.html\#//apple\_ref/doc/uid/TP40002974-CH3-SW12}.}

This operating system used Objective-C as its standard language, and proprietary libraries, called OpenStep.\footnote{Despite the name, OpenStep is not open source -- the Open allude to the fact that its API specification was open.}

Several years later, Apple (now merged with NeXT) made its new version of Mac OS, called Mac OS X; this operating system was partially based on NeXTSTEP and also used some of its proprietary libraries, now renamed Cocoa.\footnote{The kernel of Mac OS X is open source, as is its \uv{underlying} operating system called Darwin -- however, this system does not contain Cocoa libraries.}

Cocoa is not the only library for Objective-C, but because Apple is the main investor in Objective-C-based systems, it's a de-facto standard library. Cocoa is nowadays found in every Mac PC, iPhone and iPad and maybe other Apple's products.

\subsection{PC Translator}
\label{systems:langsoft}

PC Translator is a commercial translation system from a Czech company LangSoft (\url{http://www.langsoft.cz/translator.htm}). PC Translator works with several language pairs, all with Czech on either source or target side.

Authors of PC Translator don't publish any papers or other literature about the system -- what can we tell about its functionality is gathered only from its promotional website and from the experiments with the software itself.

PC translator seems to be purely rule-based. The system seems to work in following steps:

\begin{pitemize}
\item some (probably rule-based) morphological analysis of the source language
\item translation of the lemmas from source language to target language by searching in a large dictionary
\item some synthesis of morphological information and the translated lemma
\end{pitemize}

The system doesn't seem to do any kind of reordering. It also doesn't seem to do any analysis on a deeper level, like sentence constituents. Some of the phrases in the dictionary are longer than one word, but most of them seem to be one-word only.

One of the advantages of PC Translator is its large dictionary -- however, the dictionary is sometimes choosing very odd and inprobable choices when disambiguating between more possible translations. For example, the English sentence \uv{I like dogs} is translated as \uv{Mám rád kleště}, because the term \uv{dog} can be also translated as \uv{kleště}\footnote{from Collins' Dictionary: \uv{dog -- 5. a mechanical device for gripping or holding, esp one of the axial slots by which gear wheels or shafts are engaged to transmit torque}}. This can be seen as a proof that PC Translator is a purely rule-based system.

According to its marketing materials, PC Translator v14 uses a Czech-Russian dictionary with above 650.000 words.

\section{Statistical systems}

\subsection{Google Translate}
\label{systems:google}
Google Translate is a popular free online translation service by Google, an American web search giant (\url{http://translate.google.com}). 
Although Google is producing many academic papers on machine translation, the whole system is still proprietary and we cannot fully inspect it, as in the case of PC Translator, and we can only state our conjectures.

According to Google's own papers\footnote{for example \cite{och} -- F. J. Och is a head of Google Translate group in Google}, Google Translate uses mostly statistical approach to machine translation.

However, because of its purely statistical approach, it either needs huge amounts of data for every language pair, or it needs to use so-called \uv{pivot languages}\footnote{See for example \cite{koehn2010statistical}} -- in the case of Google Translate, it's usually English; specific English word order and English idioms are then re-translated into the target language and sometimes introduce downright wrong translations.

\subsubsection{API}
\label{systems:googleapi}
Google Translate, apart from being a website, has a paid translation API\footnote{\url{https://developers.google.com/translate/?hl=en}}. The API is a REST-based API which returns the translation in standard JSON; however, it also needs fairly complicated OAuth authentication.

Some unofficial libraries remove this complexity and abstract it away from the user.
One of them is called prosaically \uv{google-api-translate-java} (\url{https://code.google.com/p/google-api-translate-java/}) and is, not very unexpectedtedly, Java-based.


\subsection{Bing Translator}
\label{systems:bing}
Another available online translation service is Microsoft Translator / Bing Translator. (In Microsoft's own materials, the system is usually called Bing Translator when referring to the website and Microsoft Translator when referring to the API, however it's not very consistent. I will call the whole system Bing Translator, even when referring to the API that's called just \uv{Microsoft Translator} in the documentation.)

Bing Translator is very similar to Google Translate -- it is an online website with an easy GUI and an additional paid API. Again, the team occasionally publishes some scientific papers, but the system is again  proprietary as a whole.

In separate experiments between English and German, I found out that for some language pairs, Bing Translator does more rule-based-looking post-editation.
However the system as a whole seems statistical, similarly to Google Translate.


\subsubsection{API}
\label{systems:bingapi}
Again, Microsoft offers paid Bing Translator API (confusingly marketed as a \uv{dataset} inside Windows Azure platform).

The API is slightly more complex than Google's API because of the auto-expiring token, but Microsoft itself offers some abstracting code as an example in its documentation\footnote{\url{http://msdn.microsoft.com/en-us/library/hh454950.aspx}} in C\# and PHP.

%The pricing is slightly different in Microsoft Translator than in Google Translate, but in general is slightly cheaper. First 2 million letters are for free, next 2 million are for about 40 US dollars.

\subsection{Yandex Translate}
\label{systems:yandex}
Yandex (\url{http://www.yandex.ru}) is a Russian search portal that, according to its website\footnote{\url{http://company.yandex.com/}}, generates 61 percent of web search traffic in Russia.

Apart from being a search engine, Yandex offers a variety of other services. One of them is Yandex Translate (\url{http://translate.yandex.com})\footnote{Or \url{http://translate.yandex.ru} for Russian version} -- again, a simple website for automatized translations, similar to aforementioned Google Translate or Bing Translator.

%I wanted to include Yandex Translate, because as a Russian service, it could have better Russian language models and better Russian support in general.

\subsubsection{API}
%From the three online services, Yandex API is probably the simpliest to use. It uses a simple JSON interface, which requires an API key.
Yandex Translate also has a translation API. 
The API itself is absolutely free, unlike the other two translation systems, and is probably the easiest of the three online services to implement; however, it has strange and vaguely defined usage limits with no way of checking the actual usage.

%In our experiments, the API simply stopped returning sentences after approximately 1 million characters per 24 hours. After 24 hour period, the API became usable again.

\subsection{Moses}
\label{systems:moses}
Moses is an open-source machine translation toolkit with GPL licence, developed as a successor to a closed-source Pharaoh system. See for example \cite{mosespaper} or \cite{moseslink}.

The system is very modular and very customizable, which makes it a bit harder to describe. What makes it also harder is that the term \uv{Moses} is usually applied for both the \uv{core} Moses decoder and the phrase extractor, and the whole toolkit that's bundled with it. I will try to describe it from the point of view that's relevant to the task of this thesis and write only about the modules that I have actually used, and about my Moses use-case in general.

%In this section, I will try to describe our Moses set-up; first, I describe the overview of the entire system, and then I further describe some of the elements and our contributions.
%; bear in mind, however, that a completely different set-up is also possible.


\subsubsection{Pipeline overview}
In a very broad view on Moses pipeline, we have some corpus of texts, either parallel or monolingual, and we want to somehow learn a \emph{model} for the translation task. We can then use this model for translating any other sentences in the source language.

This is still a fairly broad definition. For our purposes, let's assume we have a bilingual corpora of a given language pair and a different, usually bigger, monolingual corpora of the target language. We can then learn \emph{translation model} from the bilignual corpora, which is responsible for the \uv{precision} of the translation; and then \emph{language model}, responsible for \uv{fluency}. The actual translation is then \uv{combining} those two factors.

The translation model is called \emph{phrase-based}, because it contains whole phrases, and it contains probabilities of their possible translation, inferred from the corpus. 
Similarly, language model contains probabilities of various word n-grams. 

%At the start, we have a bilingual corpora of a given language pair, and bigger monolingual corpora of the target language.
Now we can look a little closer to what is actually hapenning and what are the actual needed steps.

The bilingual corpora have to be first prepared by aligning the sentences, so every sentence has exactly one translation. (Almost every corpus, available online, is already sentence-aligned.)

The sentences are then word-aligned, which means pairing words to their translations. I am using MGIZA++ (\cite{mgiza}). From this word alignment, Moses learns a so-called \emph{phrase-based translation model}. From the monolingual corpora, a statistical \emph{language model} is learned -- using, for example, SRILM language model (\cite{srilm}).

Moses is then used for so-called \emph{decoding} of the information from both the language model and the translation model, which choses the best possible translation, using algorithms like beam-search.

However, for the best translation, we need to tune Moses parameters for optimal results. This is done using so-called \emph{minimum rate error rating} -- or MERT for short, which is tuning the parameters on a small separate development set.



After MERT tuning, we finally have working language model, translation model and Moses parameters, which is our complete translation system.

To reiterate,  I am using following Moses pipeline:
\begin{pitemize}
\item getting sentence-aligned parallel corpus, plus bigger monolingual corporus
\item word-alignment on parallel corpus
\item creating phrase-based translation model
\item creating language model
\item tuning the parameters for Moses decoder
\end{pitemize}

%For managing input and output from the various steps, we use \emph{eman} system, which we transformed a little.

\subsubsection{Managing experiments}
The crucial part of Moses is its decoder and phrase extractor. However, we also need some overaching system for managing all the described steps (model training, etc.) -- steps variously fail, don't compile, don't fit in memory, etc. We would also like to reuse partial results in more experiments.

Moses itself has built-in perl-based experiment managment system, called prosaically Experiment Management System (EMS). However, this system is not very widely used on UFAL and I decided to not use it.

Instead of EMS, on ÚFAL, another perl-based tool called eman (experiment manager) is used. Eman is described well in \cite{eman} or at its website, \url{http://ufal.mff.cuni.cz/eman}.  

Eman breaks down experiment into so-called \uv{steps}. Step encapsulates an atomic part of an experiment and can be in one of a few various states. More importantly, step can be dependent on various other states; if a step fails, all steps dependent on it automatically fail. The whole experiment is then just another step, dependent on all the necessary substeps.

Step is represented by a directory in a playground directory. Step is created by copying a script, called \uv{seed}, from a library of seeds, to a new directory.

%In my opinion, while eman itself is well written, I found the seeds themselves hard to read, too repetitive, and with large amount of code copied and pasted over. 

%For that reason, I tried to rewrite the seeds as perl modules instead of bash scripts for more clarity and reusability. I am, however, not personally sure if my effort in this regard was successful. I decided to use the module \texttt{MooseX::Declare}\footnote{\url{http://search.cpan.org/~ether/MooseX-Declare-0.38/lib/MooseX/Declare.pm}}, which seemed to us like a modern way to write modules in perl. 

%Unfortunately, that module is using very difficult-to-understand perl concepts and source code transformations through \texttt{Devel::Declare}, and as a result, it takes long to run and, perhaps worse, returns very confusing and undecypherable errors. 
%So as a result of my rewrite, I have seeds with code that's probably easier to read and refractor, but on the other hand, it's slow and produces very opaque errors.

%Author of \texttt{MooseX::Declare} is now recommending \texttt{Moops} module instead for declarative syntax; this module is, however, requiring perl version 14 and above, while on UFAL's network, only perl 10 is installed.

\subsubsection{Word alignment}
For word alignment, I am using MGIZA++\footnote{See \cite{mgiza}}, which is a GPL toolkit based on GIZA++\footnote{See \cite{giza}}, which is itself based on models, sometimes called IBM Model 1 to IBM Model 5\footnote{See \cite{ibm}}, which are themselves based on expectation–maximization algorithm (EM).

IBM Models and the underlying EM algorithms are explained perfectly in Chapter~4 of \cite{koehn2010statistical} or in those slides by the same author -- \url{http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/ibm-model1.pdf}.

GIZA++ is an implementation of those models. MGIZA++ is just its mu\-lti-thre\-aded variant, which makes the word alignment slightly faster.

\subsubsection{Phrase-extraction}
In this step, Moses takes the word alignment from the previous step and learns a so-called \uv{phrase table}.
Unlike word alignment, phrase extraction spans multiple words on every side in so-called \uv{phrases}.

Phrase table consists of list of phrases, their probabilities in both ways of translation, and their lexical weighting -- lexical weighting is the probability of the translated phrase counted by individual word pairs. The exact meaning of the numbers is well explained in \cite{koehn2003}.

The phrase-table defines a so-called \uv{translation model}. 

\subsubsection{Language model}
Language model is a part of the system, that tries to model the probability of a target language sentence alone. It's trained on a monolingual corpus.

I am using SRILM, which is an open source language modeling toolkit. (Although it's open-source, it uses its own license, that allows free use only for non-commercial and educational purposes.) Current status of SRILM is described in \cite{srilm}, original design is described in \cite{srilm_old}. 

SRILM uses several models, one of them is n-gram word model, described well in \cite{koehn2010statistical}\footnote{chapter 7}. I use n-grams model to the order 3 with words and order 5 with tags (see section \ref{experiments:factors}). I smooth the models with Kreser-Ney smoothing with Chen and Goodman's modification\footnote{See \cite{chen} and \url{http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html}}. 


\subsubsection{Language model interpolation}
\label{systems:interpol}
If we have more than one monolingual corpora (as I do, as described in \ref{corpora:monolingual}), but we are not sure how helpful each of them are, we can use so-called 
%As described in the section XX, we had more than one monolingual Russian corpora, but we weren't sure of how high quality each of them were and how helpful it would be. For this reason, we used so-call 
interpolation (also called mixing).

Linear interpolation in general is described for example in \cite{gutkin}. On a separate heldout data, set of \emph{lambdas} are trained -- the resulting probabilities are then just the individual probabilities, multiplied by the lambdas and summed.

Linear interpolation is supported by Moses by undocumented script in the codebase, called \texttt{interpolate-lm.perl}, which in turn uses SRILM's undocumented AWK script \texttt{compute-best-mix.gawk} and SRILM's \texttt{ngram} with \texttt{-mix-lm} option\footnote{See \url{http://www.speech.sri.com/projects/srilm/manpages/ngram.1.html}}. 
Eman manager then uses these scripts in the \texttt{mixlm} seed.

%We used a linear interpolation instead of log-linear interpolation simply because we didn't notice the option until later in the project.

\subsubsection{Factored translation}

The pipeline, described in the previous sections, translates phrases from the source language to the target language \uv{as is}. Only the exact phrases, found on the source side, can be translated to the exact phrases on the target side; and as they are decoded by Moses, only the phrases themselves are taken into account.

However, with morphologically rich languages such as Russian or Czech, this can result in worse translations because of the number of word forms and resulting data sparsity.
With so-called factored translation, we can add some morphological information while still keeping the main ideas of phrase-based translation. Factored translation was introduced in \cite{factored}.

With factored translation, 
phrased-based approach is extended with 
morphological (or other) information\footnote{Paraphrased from \cite{factored}. The exact quote is \uv{Therefore, we extended the phrase-based approach to statistical translation to tightly integrate
additional information.}}. We can add additional information (for example, lemma or morphological tag) to either side of the translation, on a word level -- this is called a \emph{factor}. Then, instead of training language models and/or translation models on the words alone, we train them on some combination of these factors and then, with the help of Moses that supports factored translation models, combine them together.

\subsubsection{Recasing}
\label{systems:recaser}
If the language and translation models are all trained on lowercased corpora (like ours are), we need to train a recaser that will convert the translated text from lower case back to upper case.

We could make a rule-based recaser, such as the ones that are included in Moses; however, we can also train a statistical recaser. 
The recaser is basically a complete Moses model, trained as a translation from lower-cased corpus to a cased corpus, where any (case-sensitive) monolingual corpus can serve as a source for the language model -- where source language is the lowercased corpus and the target language is the original corpus. 

\section{Hybrid systems}
\subsection{TectoMT}
\label{systems:tecto}
TectoMT is a translation system, developed almost exclusively at ÚFAL (\url{http://ufal.mff.cuni.cz/tectomt}; \cite{tmt_desc}). 
While it's based on linguistically motivated theory (unlike \uv{pure} statistical systems), some of its individual parts are based on statistical approaches (unlike \uv{pure} rule-based systems) -- therefore, I think it's appropriate to put it somewhere in the middle on the axis from the section \ref{axis}. Similarly to Moses in section \ref{systems:moses}, I will try to describe the general structure of the system, but only as relevant to our experiments.


TectoMT is available for a download from UFAL's public SVN repository, with the instructions on UFAL's public wiki (\url{https://wiki.ufal.ms.mff.cuni.cz/external:tectomt:tutorial}). Even more than Moses, TectoMT is an experimental software for academic usage with constant changes from many contributors -- and for that reason it takes a while to learn to use it.

\subsubsection{Treex}
TectoMT is built on the Treex platform, which used to be developed together with TectoMT under the same name, but later branched out as its own project and is nowadays used for other applications (Depfix, HamleDT). (\url{http://ufal.mff.cuni.cz/treex}). 
Still, because of the long coupled development, Treex source code and its inner structures are based on the needs of TectoMT, and even today it's sometimes difficult to say where exactly the framework ends and application begins. For example, while Treex is downloadable from CPAN perl repository, the version on CPAN is outdated and doesn't work with TectoMT; all TectoMT blocks exist under Treex package; the only way to get newest Treex sources is to install the whole TectoMT framework.

Treex and TectoMT are free software. Treex is dual-licensed under Artistic License 1.0 and GPLv2, as most CPAN packages are. TectoMT is licensed under GPLv2 outright. However, there are modules in TectoMT with more restrictive licencing -- some of them can be used only non-commercially -- and some models are trained from non-free sources and probably couldn't be used outside of academia. 


\subsubsection{Trees and layers}
Ultimatively, TectoMT is based on a linguist theory that predates machine translation by decades.

The Functional Generative Description theory comes from Prague's Linguist Circle (and its older theories), and was described for example in \cite{sgallczech} (in Czech) or \cite{sgallenglish} (in English). It describes a system of various layers of description and the system of their representation and composition, where the layers are (from the \uv{deepest} level) tectogrammatical, phenogrammatical, morphemic, morphophonemic and phonetical\footnote{\cite{sgallenglish}, page 26.}. The concept of tectogrammar was first introduced in \cite{curry}.

Some of the layers would use dependency trees, which are inspired by Czech \emph{sentence analysis} (described for example in \cite{smilauer}).

While Functional Generative Description is a theory, Prague Dependency Treebank project\footnote{\cite{pdt_soft}, the latest description in \cite{pdt_desc} and more detailed in \cite{pdt_manual_a}, \cite{pdt_manual_m} and \cite{pdt_manual_t}} is an application of this theory on actual data. Its data format and software tools are used directly in TectoMT.

PDT uses several layers, with an inspiration from FGD theory. However, instead of the many FGD layers, PDT uses the following ones:
\begin{itemize}
\item w-layer (word layer) for segmented words
\item m-layer (morphological layer), where every word has been transformed into a combination of lemma and tag; there is still no relation between words and the  structure is still \uv{flat}
\item a-layer (analytical layer), where the sentence is tranformed into a dependency tree. The edges represent constituent dependency (or some other relation) and are marked with one of 28 analytical functions (also called \emph{afun})\footnote{Technically, the edge is not marked with the function, only the dependent node.}
\item t-layer (tectogrammatical layer), that tries to express semantic structure of a sentence, again with a dependency tree. Nodes on this layer sometimes correspond to nodes on a-layer, but sometimes some artificial nodes are added and, on the other hand, auxiliary words are removed. In addition to \emph{t-lemma} (corresponding with morphological lemma), each node has a \emph{functor}, that tries to somehow convey a semantic function of a relation to node's head (for example, \texttt{AIM} as adjunct expressing purpose). Semantic morphological categories are represented by \emph{grammatemes} (for example, \texttt{number=sg} for singular).
\end{itemize}

TectoMT uses the described PDT layer logic. 
The idea of TectoMT is to first convert source sentences through all the layers to t-layer (\emph{analysis}), translating the t-layer to the target language (\emph{transfer}) and converting it back to full sentences (\emph{synthesis}).
%Originally, the plan was to convert source sentence through all the layers to t-layer, transfer the semantics and generate the target sentence.

However, TectoMT modifies the PDT model with the addition of \emph{formemes} (described for example in \cite{zabokrtsky_hab}). Formemes are added to nodes on t-layer, and represent \uv{in which morphosyntactic form the t-node was (in the case of analysis) or will be (in the case of synthesis) expressed in the surface sentence shape}\footnote{\cite{zabokrtsky_hab}}. 

Theoretically, they should be seen as something \uv{between} the t-layer and the layers above.
Example formeme is \texttt{n:since+X} for English expression of time, translatable as \texttt{n:od+2} for Czech (2 for genitive)\footnote{Example from \cite{zabokrtsky_hab}}.

Formemes are technically not \uv{correct} according to FGD description and they shouldn't be needed for analysis or synthesis, and in the tranfer phase, we should be able to just transfer the functors. 

However, the motivation for formemes (at least my understanding of it) is, that \emph{we are not that far}, \uv{pure} semantic translation is not possible with our tools, and it's better to transfer t-lemmas, formemes and grammatemes, and generate the more surface layers from that.

Getting back to Russian language, with respect to PDT layers -- while some preliminar research has been done into representing Russian according to the described model (\cite{syntagrus_pdt}), the application of PDT theory into Russian is far from complete.

\subsubsection{Blocks}

Every TectoMT task can be decomposed into so-called \emph{blocks}. To quote \cite{zabokrtsky_hab}: \uv{The basic processing units are blocks. They serve for some very limited, well defined, and often linguistically interpretable tasks (e.g., tokenization, tagging, parsing).} In other words, block is given a tree as an input and the block then does some well-defined task on it.

More blocks in a row are called a \emph{scenario}.

Blocks can be language specific or language agnostic; simple blocks like copying a tree are usually language agnostic, as well as other parametrizable blocks -- however, most blocks are usually language specific.

Because of language specificity of most of the blocks, scenarios for various language pairs are language specific too. For example -- while it's not up-to-date, English-to-Czech scenario is thoroughly described in \cite{zabokrtsky_hab}.

\subsubsection{Makefiles}

TectoMT package itself also contains various scripts and tools -- one of them are Make scripts for easier running and evaluation of the scenarios\footnote{What needs to be said is that's it's tailored mostly for UFAL's cluster infrastructure instead of for general usage.}. The goals of those scripts are different from Moses evaluation managers, described in \ref{systems:moses}, as well as it means, but we can imagine it as being slightly similar. (Those Make scripts don't, unfortunately, have any nice name to refer to them as.)

\subsubsection{MaxEnt, HMTM}
Interesting techniques of using Hidden Markov Tree Models and Maximum Entropy models for TectoMT translation are described in \cite{tecto_hmtm} and \cite{tecto_maxent}. Unfortunately however, I haven't been able to train any of those for Czech-to-Russian translation.

\subsubsection{Current Czech-to-Russian scenario}
\label{systems:currenttecto}
Unlike Moses where I had to had to train whole models from zero, TectoMT already had some rudimentary scenario for Czech-to-Russian machine translation.


From what I heard from colleagues, this scenario was put together in a very short timeframe (about 24 hours), and it's not described in any paper or any other documentation. The scenario consists of 66 blocks, where Czech analysis is 33 blocks, transfer is 5 blocks and Russian analysis is 28 blocks. 

While I have to admit I don't have enough experience to judge it, Czech analysis seems to be well done and probably copied from some other working scenario.

However, the transfer and Russian synthesis doesn't seem to be very advanced. Transfer of \uv{t-lemmas} actually only involves look-up in a very small dictionary, made by taking the proprietary PC-Translator dictionary (section \ref{systems:langsoft}) and taking a subset of its lexical lemmas that also appear in UMC (section \ref{corpora:umc}). Unlike more advanced models, in this model, any given lemma is always translated the same way, no matter the context. Similarly, transfer of formemes is just a copy and a few hand-written rules.

With regards to Russian synthesis -- about half of its blocks seem to be Czech synthesis blocks.
This can be somehow justified by the similarity of the two languages, but it also doesn't exactly inspire confidence.


In general, it's obvious the scenario was done very quickly and, to me, it's quite surprising it's even translating something.\footnote{Although, as will be seen in the chapter \ref{chapter:results}, the results are not that good.}
