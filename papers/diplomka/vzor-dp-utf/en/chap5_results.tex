\chapter{Results}

\section{Overview}
\label{overvieweval}
I am testing all the following systems:
\begin{pitemize}
\item PC Translator \emph{(rule-based, TODO)}
\item Google Translate \emph{(statistical, TODO)}
\item Microsoft Translator \emph{(statistical, TODO)}
\item Yandex Translate \emph{(statistical, TODO)}
\item Moses \emph{(mostly statistical, TODO)}
\item Treex \emph{(hybrid, TODO)}
\end{pitemize}

I am testing on two separate test sets: WMT 2013 and Filtered Intercorp, as described in the section TODO.

I have randomly selected 10 sentences, 5 from each set, to allow readers to compare the system for themselves; the results are in the chapter \ref{sampleofresults}.

Unfortunately, I cannot rule out the possibility that Google, Yandex or Bing Translator already have WMT 2013 sentences, or at least some of them, in their training data, as they have been public for about a year when I run the tests. It's less likely that they trained on Intercorp data -- however, as they are black-box systems, we can never tell for sure.

%First, I will demonstrate the six systems on 10 randomly selected sentences, to allow readers to compare the systems for themselves. Then I will compare the systems using some automatical metrics. Then I will talk about my (largely unsuccessful) attempts at human annotations; and at the end, NĚCO SE MNOU BUDE


\section{Automated metrics}
\jednatabulkan{intermetrics} { |r|r|r |r|r|r| } {

\hline &  BLEU  &  WER  &  TER  &  CDER  &  PER \\ \hline

Yandex & 11.65 & 24.47 & 25.77 & 31.95 & 38.63\\ \hline
Moses & 11.62 & 26.73 & 28.26 & 32.98 & 43.65\\ \hline
Google & 8.79 & 21.22 & 22.35 & 27.36 & 37.39\\ \hline
Bing & 7.22 & 19.93 & 21.11 & 25.82 & 36.51\\ \hline
TectoMT & 5.81 & 16.46 & 17.87 & 25.76 & 31.20\\ \hline
PC Translator & 5.02 & 17.99 & 18.85 & 25.06 & 31.54\\ \hline
Baseline & 0.77 & 8.36 & 8.61 & 13.23 & 19.30\\ \hline


} {Automated metrics on Filtered Intercorp}

\jednatabulkan{wmtmetrics} { |r|r|r |r|r|r| } {


\hline &  BLEU  &  WER  &  TER  &  CDER  &  PER \\ \hline
Yandex & 19.55 & 34.55 & 36.63 & 40.99 & 49.02\\ \hline
Google & 17.96 & 33.58 & 35.58 & 38.64 & 48.35\\ \hline
Moses & 17.44 & 33.20 & 35.18 & 38.81 & 49.14\\ \hline
Bing & 15.49 & 30.80 & 32.93 & 36.22 & 46.71\\ \hline
TectoMT & 8.80 & 25.00 & 26.68 & 30.45 & 41.47\\ \hline
PC Translator & 6.74 & 21.41 & 22.60 & 26.75 & 35.48\\ \hline
Baseline & 0.83 & 10.93 & 11.26 & 14.32 & 20.14\\ \hline


} {Automated metrics on WMT 2013}


I compared the systems using several automated metrics, all of them implemented in Moses internal evaluator, and all of them described well in \cite{matous}. Specifically, they are BLEU, WER, TER, CDER and PER.

For a comparable BLEU metric, I re-tokenize both reference and the tested system by Moses' built-in tokenizer skript. I also normalize punctuation, using script from WMT pages. 
I decided to use case-sensitive BLEU -- that means that words \emph{Кристиан} and \emph{кристиан} are two different words.

As a baseline, I use a transliteration GOST 7.79 RUS, mentioned in TODO.

The scores are in tables \ref{tabulka:intermetrics} and \ref{tabulka:wmtmetrics}, sorted by BLEU.

\subsection{Discussion}
The first notable thing is that the various metrics on a single corpus rougly agree on the order, except for small differences -- for example, BLEU on Intercorp puts Yandex as the best system, unlike the other metrics, that put our Moses on top. Similarly, with WMT data, our Moses and Google would switch second and third place, depending on the metric.

What was slightly surprising for me was the results of Yandex Translate. I have originally added Yandex to the list of systems only for \uv{completeness}, but it actually outperformed Google Translate and my Moses setup. In retrospect, this makes sense -- Yandex is a Russian company and, as such, probably has better statistical models of Russian and a better morphology.

We can also note that the only purely rule-based system -- PC Translator -- always ended up last, hybrid system came out slightly better and more statistical systems came out the best (altough we \emph{do} use morphological tags in Moses in the language model, as described in TODO). It might be seen as the proof that statistical systems have better results; however it can also be seen as the proof that the metrics are better suited to statistical systems.

\subsubsection{Difference between test corpora}

\jednatabulkan{corpdif} { |r|r|r |r| } {

\hline &  Intercorp  &  WMT  & $\div$   \\ \hline

Bing & 7.22 & 15.49 & 2.15\\ \hline
Google & 8.79 & 17.96 & 2.04\\ \hline
Yandex & 11.65 & 19.55 & 1.68\\ \hline
TectoMT & 5.81 & 8.80 & 1.51\\ \hline
Moses & 11.62 & 17.44 & 1.50\\ \hline
PC Translator & 5.02 & 6.74 & 1.34\\ \hline
Baseline & 0.77 & 0.83 & 1.08\\ \hline

} {Differences between BLEU on two test corpora}

What is also interesting is how much the results on Intercorp and WMT test data differ from each other, as seen in the Table~\ref{tabulka:corpdif}, ordered by the quotient.  

Every system had better results on WMT than on Intercorp, even our baseline. However, Bing and Google had almost twice as good BLEU on WMT than on Intercorp, while our systems were more \uv{stable}.

This is probably caused by the fact, that Google and Bing train their models on more publicly accessible news data, while I added some prose to Moses parallel data along with the news. I still have mostly news data in the language model; if I used the TODO mentioned in TODO, I could maybe get a better results on Intercorp.

This all, however, begs more theoretical question. 
Is it right that we, as MT researchers, mostly test our systems on news data, as for example in all WMT translation tasks? 
Shouldn't we broad the domain a bit, to include fiction, and maybe other literature? 

It's possible that with heavy accent on parallel news data, we are skewing the translation systems so that they translate news articles well, but are significantly worse on other type of data. Should we strive for more general translation systems, or for translation systems, that do one type of text well?

This is an open question, and I don't claim to have an easy answer. So far, every WMT translation task has been using news test data, and it doesn't seem this will change in the near future.

\section{Human evaluation}

%Human evaluation experiment was largely unsucessful.

Originally, I planned to use Appraise system, used for human evaluation at WMT (\cite{appraise}), and afterwards use the TrueSkill algorithm (\cite{trueskill}), that was used in WMT 2014 as the best method for ordering systems based on human annotation (\cite{wmt_findings_2014}). 

Appraise ended up being too hard to install and set-up\footnote{Appraise is a Django application, and I have almost no experience with either Django or Python}, so I ended up writing my own simple PHP application, heavily inspired by Appraise. 

Similar to Appraise, the application presents source sentence and a reference sentence to users, and
asks them to rate them from best to worst.
Unlike Appraise, that randomly chooses 5 systems every time, I used the fact that we have 6 systems and present all 6 to them to users.

I put the systems online and invited several people and tried to advertise it. However, with little funds and little time to experiment with the incentives, I decided to not pay anything for the annotation and let the user decide, how much he wants to annotate.

This lead to only 38 annotated sequences in total.

\begin{comment}
%We also replace \texttt{\&quot;}, that appears often in Intercorp, with actual quotation marks.


\subsubsection{Human evaluation}
TODO: moje plány s TrueSkill/Appraise, co nevyšly, protože mi to nikdo neanotoval. Za celou dobu mi tam 4 lidé odanotovali asi 13 vět.

\subsection{Baseline}


\jednatabulkan{bleubase} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 0.77\% $\pm$ 0.06\%
&
0.83\% $\pm$ 0.16\%

\\ \hline
}{Baseline BLEU}


Resulting BLEU is very low, as expected.

\subsubsection{Discussion}
We can see the results are not ideal. The sentences are evidently translated word-by-word, without any regard for the whole sentence, or the correct word order.

Because the languages are similar, the morphological information is sometimes transferred correctly -- for example, \uv{s hrdostí vzpomněl} -> \uv{с гордостью вспомнил} -- but sometimes, very incorrectly -- for example, genitive \uv{překrásné} is not with the correct ending as \uv{-ой}, but incorrectly as \uv{-ое}.

It's obvious the dictionary is quite big and covering a lot of the words -- most of the words have at least some translation -- but the translation is usually wrong, or used in a wrong context.



\subsubsection{BLEU}
\jednatabulkan{bleugoog} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 8.79\% $\pm$ 0.15\%
&
17.96\% $\pm$ 0.58\%

\\ \hline
}{Google Translate BLEU}

\subsubsection{Discussion}
As I already noted, it's much harder for me to analyze the phrase-based translations by only looking at them, since they \uv{seem correct}, since they are put together from more correct sentences and are \uv{locally correct}.

However, if we look at the BLEU, we can notice an interesting phenomenon, that will be seen at other systems too, and I am discussing it further below -- the BLEUs of the two systems are significantly different. In fact, BLEU on WMT is two times bigger than on Intercorp.

I am discussing this phenomenon further in the section TODO.
\subsubsection{BLEU}

\jednatabulkan{bleubing} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 7.22\% $\pm$ 0.13\%
&
15.49\% $\pm$ 0.50\%

\\ \hline
}{Microsoft Translator BLEU}

\subsubsection{Discussion}
Again, I cannot comment on the translation quality so much as I would like to, but we can see the same BLEU disprepancy between the two corpora, as I am further discussing in TODO.

\subsubsection{BLEU}

\jednatabulkan{bleuyandex} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 11.65\% $\pm$ 0.16\%
&
19.54\% $\pm$ 0.54\%

\\ \hline
}{Yandex Translate BLEU}

\subsubsection{Discussion}
Yandex is the most interesting from the three \uv{online translators}.

Originally, we didn't plan on including it in the experimens, but we added it in the end \uv{just in case} -- and it ended up being the best translation system from the three.

And it's interesting that it ends up being the best system \emph{despite} a lot of Czech words being untranslated and ending up as-is on the Russian side.

The system is closed-source, so we can't tell what are the exact reasons for this system being the best; however, it's obvious that Yandex is a Russian company and, therefore, probably have the best available Russian language models and morphological tools.

It also seems like the translation is \emph{not} using English as a pivot language -- there doesn't seem to be any anglicism in the text. 


\subsubsection{BLEU}

\jednatabulkan{bleumoses} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 11.62\% $\pm$ 0.15\%
&
17.45\% $\pm$ 0.51\%

\\ \hline
}{Moses BLEU}

\subsection{Problem with my evaluation}
My original plan was to do both automated testing \emph{and} do some human judgement on the results, as I do have some basic Russian language skills and I am fully fluent in CZech.

However, I soon found out my level Russian understanding is not high enough for comparing various automated systems. Therefore I will talk about the translation quality only at a very surface level.

At this point, the reader might ask -- why spend time on Czech-to-Russian translation with only passing knowledge of Russian, when Russian-to-Czech translation would be easier for me to evaluate?


I agree that this question has some merit. However, I chose Czech-to-Russian translation pair for several reasons:
\begin{pitemize}
\item existing historical systems like RUSLAN, that I planned to compare with modern systems. Unfortunately, I wasn't able to make any of the systems work.
\item pre-existing Czech-to-Russian TectoMT scenario. I was actually to utilize that and build on it, instead of making the system from scratch, as it would be needed.
\end{pitemize}

%My original plan was that, apart from doing automatized testing, I would be able to look at the results, compare the translation and by comparing the original translation and the translated sentences, I would be able to give some insights on the systems -- as I can personally read Cyrillics, I have some basic understanding of a Russian language and I am a native Czech speaker. However, it turned out that for judging translation errors and classifying them, one needs to know the language on an advanced level -- especially with the phrase-based translation, that makes the sentences from feasibly looking phrases. For this reason, I will abstain for saying too much about the quality of every system.

\end{comment}

