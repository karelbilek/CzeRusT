\chapter{Results}
\label{chapter:results}

\section{Overview}
\label{overvieweval}
I am testing all the following systems:
\begin{pitemize}
\item PC Translator \emph{(rule-based, \ref{systems:langsoft}, \ref{experiments:langsoft})}
\item Google Translate \emph{(statistical, \ref{systems:google}, \ref{experiments:google})}
\item Bing Translator \emph{(statistical, \ref{systems:bing}, \ref{experiments:bing})}
\item Yandex Translate \emph{(statistical, \ref{systems:yandex}, \ref{experiments:yandex})}
\item Moses \emph{(mostly statistical, \ref{systems:moses}, \ref{experiments:moses})}
\item TectoMT \emph{(hybrid, \ref{systems:tecto}, \ref{experiments:tecto})}
\end{pitemize}

I am testing on two separate test sets: WMT 2013 and Filtered Intercorp, as described in the section \ref{corpora:parallel}.

I have randomly selected 10 sentences, 5 from each set, to allow readers to compare the system for themselves; the results are in the section \ref{sampleofresults}.

Unfortunately, I cannot rule out the possibility that Google, Yandex or Bing Translator already have WMT 2013 sentences, or at least some of them, in their training data, as they have been public for about a year when I run the tests. It's less likely that they trained on Intercorp data -- however, as they are black-box systems, we can never tell for sure.

%First, I will demonstrate the six systems on 10 randomly selected sentences, to allow readers to compare the systems for themselves. Then I will compare the systems using some automatical metrics. Then I will talk about my (largely unsuccessful) attempts at human annotations; and at the end, NĚCO SE MNOU BUDE


\section{Automated metrics}
\jednatabulkan{intermetrics} { |r|r|r |r|r|r| } {

\hline &  BLEU  &  WER  &  TER  &  CDER  &  PER \\ \hline

Yandex & 11.65 & 24.47 & 25.77 & 31.95 & 38.63\\ \hline
Moses & 11.62 & 26.73 & 28.26 & 32.98 & 43.65\\ \hline
Google & 8.79 & 21.22 & 22.35 & 27.36 & 37.39\\ \hline
Bing & 7.22 & 19.93 & 21.11 & 25.82 & 36.51\\ \hline
TectoMT & 5.81 & 16.46 & 17.87 & 25.76 & 31.20\\ \hline
PC Translator & 5.02 & 17.99 & 18.85 & 25.06 & 31.54\\ \hline
Baseline & 0.77 & 8.36 & 8.61 & 13.23 & 19.30\\ \hline


} {Automated metrics on Filtered Intercorp}

\jednatabulkan{wmtmetrics} { |r|r|r |r|r|r| } {


\hline &  BLEU  &  WER  &  TER  &  CDER  &  PER \\ \hline
Yandex & 19.55 & 34.55 & 36.63 & 40.99 & 49.02\\ \hline
Google & 17.96 & 33.58 & 35.58 & 38.64 & 48.35\\ \hline
Moses & 17.44 & 33.20 & 35.18 & 38.81 & 49.14\\ \hline
Bing & 15.49 & 30.80 & 32.93 & 36.22 & 46.71\\ \hline
TectoMT & 8.80 & 25.00 & 26.68 & 30.45 & 41.47\\ \hline
PC Translator & 6.74 & 21.41 & 22.60 & 26.75 & 35.48\\ \hline
Baseline & 0.83 & 10.93 & 11.26 & 14.32 & 20.14\\ \hline


} {Automated metrics on WMT 2013}


I compared the systems using several automated metrics, all of them implemented in Moses internal evaluator, and all of them described well in \cite{matous} (Czech). Specifically, they are BLEU, WER, TER, CDER and PER.

For a comparable BLEU metric, I re-tokenize both reference and the tested system by Moses' built-in tokenizer skript. I also normalize punctuation, using script from WMT pages. 
I decided to use case-sensitive BLEU -- that means that words \emph{Кристиан} and \emph{кристиан} are two different words.

As a baseline, I use a transliteration GOST 7.79 RUS, mentioned in \ref{sampleofresults}.

The scores are in tables \ref{tabulka:intermetrics} and \ref{tabulka:wmtmetrics}, sorted by BLEU.

\subsection{Discussion}
The first notable thing is that the various metrics on a single corpus rougly agree on the order, except for small differences -- most notably, on Intercorp, Yandex and our Moses would switch places, depending on the metric. Similarly, with WMT data, Moses, Yandex and Google would switch places, depending on the metric.

What was slightly surprising for me was the results of Yandex Translate. I have originally added Yandex to the list of systems only for \uv{completeness}, but it actually outperformed Google Translate and my Moses setup. In retrospect, this makes sense -- Yandex is a Russian company and, as such, probably has better statistical models of Russian and a better morphology.

We can also note that the only purely rule-based system -- PC Translator -- always ended up last, hybrid system came out slightly better and more statistical systems came out the best (altough we \emph{do} use morphological tags in Moses in the language model, as described in \ref{experiments:factors}). It might be seen as the proof that statistical systems have better results; however it can also be seen as the proof that the metrics are better suited to statistical systems.

\subsubsection{Difference between test corpora}

\jednatabulkan{corpdif} { |r|r|r |r| } {

\hline &  Intercorp  &  WMT  & $\div$   \\ \hline

Bing & 7.22 & 15.49 & 2.15\\ \hline
Google & 8.79 & 17.96 & 2.04\\ \hline
Yandex & 11.65 & 19.55 & 1.68\\ \hline
TectoMT & 5.81 & 8.80 & 1.51\\ \hline
Moses & 11.62 & 17.44 & 1.50\\ \hline
PC Translator & 5.02 & 6.74 & 1.34\\ \hline
Baseline & 0.77 & 0.83 & 1.08\\ \hline

} {Differences between BLEU on two test corpora}

What is also interesting is how much the results on Intercorp and WMT test data differ from each other, as seen in the Table~\ref{tabulka:corpdif}, ordered by the quotient.  

Every system had better results on WMT than on Intercorp, even our baseline. However, Bing and Google had almost twice as good BLEU on WMT than on Intercorp, while our systems were more \uv{stable}.

This is probably caused by the fact, that Google and Bing train their models on more publicly accessible news data, while I added some prose to Moses parallel data along with the news. I still have mostly news data in the language model; if I used the Lib.ru data mentioned in \ref{corpora:libru}, I could maybe get even better results on Intercorp.

This all, however, begs a more theoretical question. 
Is it right that we, as MT researchers, mostly test our systems on news data, as for example in all WMT translation tasks? 
Shouldn't we broad the domain a bit, to include fiction, and maybe other literature -- and possibly even more kind of data?

It's possible that with heavy accent on parallel news data, we are skewing the translation systems so that they translate news articles well, but are significantly worse on other type of data. Should we strive for more general translation systems, or for translation systems, that do one type of text well?

This is an open question, and I don't claim to have an answer. I don't have the type of data Google probably has, so I don't know what exactly are users translation and in what amounts.

It's also mostly a rhetorical question. So far, every WMT translation task has been using news test data, and it doesn't seem this will change in the near future.

\section{Human evaluation}
\label{results:human}

%Human evaluation experiment was largely unsucessful.

\subsubsection{Appraise, TrueSkill}
Originally, I planned to use Appraise system, used for human evaluation at WMT (\cite{appraise}), 
and afterwards feeds its output to the TrueSkill algorithm (\cite{trueskill}), that was used in WMT 2014 as the best method for ordering systems based on human annotation (\cite{wmt_findings_2014}). 

\subsubsection{My PHP system}

Appraise ended up being too hard to install and set-up\footnote{Appraise is a Django application, and I have almost no experience with either Django or Python}, so I ended up writing my own simple PHP application, heavily inspired by Appraise. 

Similar to Appraise, the application presents source sentence and a reference sentence to users, and
asks them to rate them from best to worst.
Unlike Appraise, that randomly chooses 5 systems every time, I used the fact that we have 6 systems and present all 6 to them to users. Sentences are took randomly from the two test sets.\footnote{Because of the difference in size of the sets, the set is selected first with 0.5 probability, and only then the sentence itself.}


\subsubsection{Results}
\grafff{cluster3}{TrueSkill results}{140}

I put the systems online and invited several people and tried to advertise it. However, with little funds and little time to experiment with the incentives, I decided to not pay anything for the annotation and let the user decide, how much he wants to annotate.

This led to only 38 annotated sequences in total.\footnote{For some reason, TrueSkill percieves this as 571 judgements; I am not sure how was this number created.} This is not very much; however, one of the described features of TrueSkill algorithm is that it doesn't need that many judgements, so I decided to run it anyway.

The results are on Figure~\ref{graf:cluster3}. Moses is put as the best system into its own cluster, while the rest is put into another cluster. In the second cluster, PC Translator is probably the worst, but other than that we can't really say much. 

I am not sure how much does this result tell us, given that it's only 38 sentences in total. Looking back, I should have probably made the task easier instead of harder (by giving less choices, not more choices), and I should have offered some financial incentives for annotators.

\subsubsection{Some comments from the annotators}
While I did not have that many annotators, they gave me some some comments about the test in general. I thought they would be interesting to note here.

\begin{itemize}
\item Some of the sentences are too long, which makes the judgements too hard.
\item The translations seem either all correct or all wrong.
\item The reference translation is either completely wrong (wrong alignment) or not a literal translation, which confuses the annotator.
\end{itemize}

The first comment is probably caused by the fact that I had used prose along with news articles. It would be possible to limit the annotation to shorter sentences, but then the annotation wouldn't be entirely accurate, since the shorter sentences are usually better translated in general.

The second comment is probably caused by the nature of MT tasks in general.

The last comment is the unfortunate side-effect of either misalignment or just less literal translations. It might be possible to \emph{not} show the reference translation at all; I am not sure if that would get the judgements easier or harder.

\section{Some observation about typical mistakes}
%I am not a native speaker of Russian. However, I can read cyrillic alphabet and I have some basic knowledge of the language; I originally thought that, 
I haven't done any systematic classification of the mistakes. I am not a native speaker of Russian, and I found out that spotting translation errors is not an easy task for a person with only passing knowledge of a language, even when the correct, reference translations are available.\footnote{In retrospect, my assumtion that spotting errors without really knowing the language will be doable has been very naive.}

I have originally planned to use human annotators for error classification. However, with the low annotator activity in an arguably easier task of ranking the translation quality (\ref{results:human}), I gave up on that idea too.

As far as I know, systematic evaluation of machine translation mistakes in this concrete language pair is currently done by my colleague Natalia Klyueva, who is a native Russian speaker and, at the time of writing this thesis, PhD student on ÚFAL.

I have, however, done some unsystematic observations about the various systems and their mistakes. All the examples in this section are from the test set, but most of them are not in the section \ref{sampleofresults}.
\subsubsection{PC Translator}
PC Translator is, even to a person with only a passive knowledge of the language, 
obviously the worst of the six systems. 
The sentences are translated literally word for word, with no respect for differing grammar of the two languages. 

For example, take the following PC Translator translation (not a complete sentence):
\priklady {
\priklad{
Opatrnost je ovšem na místě například
}{
Осторожность есть конечно на месте например}{
Ostorozhnost` est` konechno na meste naprimer}
}

PC Translator ignores that the word \uv{je} (to be) is not usually directly translated to Russian, and it just keeps the word in the sentence. 

Both Yandex and Moses translates this phrase correctly:
\priklady {
\priklad{
Opatrnost je ovšem na místě například
}{
Осторожность, однако, на месте, например
}{
Ostorozhnost`, odnako, na meste, naprimer
}
}

Those mistakes seem like a direct consequence of the system design and I am not sure we can use it in any major way, except maybe for directly copying the dictionary (as I did in TectoMT, see \ref{experiments:tecto}). 
\subsubsection{Strange TectoMT mistakes}
TectoMT returns sentences with \uv{strange} mistakes, that I cannot fully grasp or understand. 

As a first mistake, it has strange problems with punctiation. This can seem like a small problem, but each punctuation mark is an extra word in all the metrics.

For example, see this translation:

\priklady{
\priklad{
Může říct: "Změníme ten zákon a povolíme to", ale nemůže říct, že budeme dělat, jako že nic.}{
Он сказать , : » , мы изменим это закон и , , мы разрешить это » но , он не сказать , что мы делаем как .
}{
On skazat` , : » , my' izmenim e`to zakon i , , my' razreshit` e`to » no , on ne skazat` , chto my' delaem kak .
}}

You can see that the resulting punctuation is very chaotic.

Also, auxiliary words are often inserted at wrong places, as \uv{и} (and) in the following translation:

\priklady{
\priklad{
Chvíli váhal a pak se přece jen vydal zpátky a zazvonil .}{
Минуту он колебался и , , потом же просто он отправитьсяся вернуться и , он позвонил .}{
Minutu on kolebalsya i , , potom zhe prosto on otpravit`syasya vernut`sya i , on pozvonil .
}}

Some of the mistakes of TectoMT are caused by the fact that the transfer is trained (as mentioned in \ref{experiments:tecto}) on lexical lemmas, while TectoMT uses t-lemmas. For example, in the following translation:

\priklady{
\priklad{
Existují mezi USA a mnoha evropskými národy názorové rozdíly?
}{
Существуют между Преследуешь и между многими европейскими на\-ро\-да\-ми мнения разница ?
}{
Sushhestvuyut mezhdu Presleduesh` i mezhdu mnogimi evropejskimi narodami mneniya raznicza ?
}}

I personally have no idea how \uv{USA} got translated as \uv{Преследуешь} (stalking); however, it's translated like that in every sentence where it appears. 

In my opinion, most of TectoMT mistakes are caused mainly by relative unstability of the scenario, and the fact that only the most simple models are used for transfer and generation. 
I believe further refinements of the models could make the translation better by far.
\subsubsection{Untranslated words in online systems}
None of the online statistical systems seem to solve the OOV problem by transliteration. Surprisingly even Yandex, that had the best resulting BLEU, usually keeps more words in Czech than Moses. 

This might be the reason why in human anotation, Moses had better results, than Yandex; however, this is only a conjecture.

For example, take this Yandex translation:

\priklady{
\priklad{
Vítejte , sousede ! 
}{
Добро пожаловать , sousede ! 
}{Dobro pozhalovat` , sousede ! 
}}

and compare with Moses translation:

\priklady{
\priklad{
Vítejte , sousede ! 
}{
Добро пожаловать , сосед !
}{
Dobro pozhalovat` , sosed !
}}
\subsubsection{Lost negative}
\uv{Classic} mistake of statistical machine translation is the lost (or reversed) negativity. We can see it in our test set at places, for example with Google's translation (not a complete sentence):
\priklady{
\priklad{
Postavy v dramatech nemluví slangem
}{
Символы в драмах говорить сленг
}{Simvoly' v dramax govorit` sleng}}

However, this mistake is acually much less common than I anticipated. For example, Moses translated the phrase more correctly\footnote{Only concerning the negative.}:
\priklady{
\priklad{
Postavy v dramatech nemluví slangem
}{
В драме не говорит - сленг 
}{
V drame ne govorit - sleng
}}
\subsubsection{Language pivoting}
The previous demonstration is also a proof of another classical mistake, caused by using English as a pivot language. 
\priklady{
\priklad{
Postavy v dramatech nemluví slangem
}{
Символы в драмах говорить сленг
}{Simvoly' v dramax govorit` sleng}}



The word \uv{Postavy} is first translated to English as \uv{Characters} and only then back to Russian as \uv{Символы}. This mistake only appears in Google Translate and Bing Translator; it doesn't appear in our systems (because we don't use pivot languages), and it doesn't appear in Yandex Translate.


