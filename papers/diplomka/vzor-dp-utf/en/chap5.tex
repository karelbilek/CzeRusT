\chapter{Moses}

\section{General description}
Moses is an open-source machine translation toolkit with GPL licence, developed as a successor to the closed-source Pharaoh system.\footnote{See for example \cite{mosespaper} or \cite{moseslink} -- but Moses is used so often and so extensively that many other papers could be found}

The system is very modular and very customizable, which makes it a bit harder to describe. In this section, I will try to describe our Moses set-up; bear in mind, however, that a completely different set-up is also possible.


\subsection{Pipeline overview}
At the start, we have a bilingual corpora of a given language pair, and bigger monolingual corpora of the target language.

The bilingual corpora has to be prepared by aligning the sentences, so every sentence has exactly one translation. We describe our corpora in the part ???

The sentences are then word-aligned, which means pairing words to their translations. We are using MGIZA++\footnote{See \cite{mgiza}}.

From this word alignment, Moses learns a so-called \emph{phrase-based translation model}.

From the monolingual corpora, we then learn a statistical language model. We use SRILM language model\footnote{See \cite{srilm}}.

Moses is then used for so-called \emph{decoding} of the information from both the language model and the translation model, which choses the best possible translation, using things like beam-search.

However, for the best translation, we need to tune Moses parameters for optimal results. This is done using so-called \emph{minimum rate error rating} -- or MERT for short, which is tuning the parameters on a small separate development set.

After MERT tuning, we finally have working language model, translation model and Moses parameters, which is our complete translation system.

To reiterate, this is our Moses pipeline
\begin{pitemize}
\item getting sentence-aligned parallel Czech-Russian corpus, plus Russian monolingual corporus
\item world-alignment on parallel corpus
\item creating phrase-based translation model
\item creating Russian language model
\item tuning the parameters for Moses decoder
\end{pitemize}

For managing input and output from the various steps, we use \emph{eman} system, which we transformed a little.


\section{Our Moses setup}
In this section, I am describing our exact setup and our data.

\subsection{Parallel data}
\subsubsection{Intercorp}
In general, we described Intercorp corpus in \ref{intercorp_p1}.


We created the Moses system before we started evaluating the system on the evaluation corpus, as described in Section \ref{intercorp_p1}. 
The data we had access to were more limited at that time: they were from the previous version of Intercorp, didn't have all the metadata, such as the publishing dates and information about the original translation source, and were shuffled. However, we can reconstruct the metadata by comparing this shuffled data with the newest version of intercorp.


TODO: popis intercorp dat

\subsubsection{Subtitles}
Another set of data that we used were subtitles from OpenSubtitles database.

In a separate FilmTit project\footnote{???citovat ročníkový projekt/dokumentaci k němu?}, we tried to make a machine translation project for subtitle translation from English to Czech.

For that project, we were given access to the set of subtitles from the server OpenSubtitles (\url{http://opensubtitles.org}). This dataset was not a pair of aligned sentences, not even a pair of aligned \emph{files}; we were given just a set of SRT files, and a table which paired each of those files with a movie (identified by IMDB number). Each movie usually has more subtitle files.

Subtitle files have the sentences paired with timestamps.

From a set of SRT files paired with a movie, we selected just one Czech and just one English SRT file which we find most similar, based on the timestamps.
From the pair of the files, we then extract the sentences that have the most similar timestamps.

These two pairings -- pairings of subtitle files and pairing of the actual lines -- are non-trivial task, and require a \uv{tolerance} -- how different can the times of a sentence be to be still paired together.

Higher tolerance produces bigger corpus with more errors, while lower tolerance produces smaller, but more correct corpus.

When we were experimenting on the afforementioned project, we found out, that the best results (tested both on another movie subtitles and a different corpus) are -- without exception -- with \emph{bigger corpus} and \emph{higher tolerance}. Even when that introduced a lot of incorrect sentence pairs, the overall results were still better with the biggest possible corpus.

For Czech-to-Russian experiments, we were given another, similar set of subtitels from OpenSubtitles. The set was naturally much smaller than with English and Czech, but we were still able to use the same algorithms to build a parallel corpora.

We again used the highest possible tolerance, and therefore surely introduced a lot of errors. Unfortunately, for a lack of time, we weren't able to replicate the experiments for the ideal tolerance here, however, we hoped that the results would be similar than in English-to-Czech translation, that is, the biggest corpus resulting in the best translation.

\subsubsection{UMC corpus}
UMC (UFAL Multilingual Corpus) is a Czech-English-Russian corpus\footnote{See \cite{umc}}. The data are downloaded from Project Syndicate, a Prague-based non-profit news organization, translating news and opinions from around the world.

TODO: počty vět a tokenů, jediná věc, co o UMC můžu napsat

\subsubsection{Wiki titles}
We also extracted all of the titles from the Czech and Russian wikipedia, that correspond to each other.

Those are usually only noun phrases and the main word is usually in nominative singular, so the morphology isn't that rich -- however, we hope that the model will learn some phrases needed for the translation of the named entities.

\subsection{Monolignual data}
\subsubsection{News Crawl}
The largest part of our monolingual data is corpus from WMT workshops that they call \uv{News Crawl}.

According to \cite{wmt_findings_2009}, WMT workshop has been continously crawling web articles since 2007 for making test sets. This allowed them to make a big, randomized corpus from all these sources.

The corpus is categorized by year, and we treat each year as its own corpus for the interpolation.
\subsubsection{Common Crawl}
Common Crawl is publicly available web crawl\footnote{From Wikipedia -- \uv{A Web crawler is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing. }; web crawl is then a result of such a web crawler} -- \url{http://commoncrawl.org/}.

As described in \cite{commoncrawl}, group of researchers tried to extract parallel data from this web crawl. One of the language pairs was English-Russian and the result is publicly available on WMT site. We used the Russian side of the corpus.

However, the quality of this corpus is very discutable. Because it contains data downloaded from the \uv{raw web}, it often has sentences in different languages, sentences in machine-translated Russian, random UTF-8 symbols, random HTML data, some code, and so on.

This was one of the reasons why we decided to use linear interpolation as discussed above -- hoping, that the tuning algorithm will automatically \uv{find the right balance} between the language models.

\subsubsection{Yandex}
%Yandex is a Russian portal and search service, that can be described like a Russian mix between Google and Yahoo!. One of its services is a free translation service
Yandex was already described in the chapter ??. Apart from providing free translation API, Yandex also provides an English-Russian parallel corpus.

We used the Russian part of this corpus for language modelling.

\subsection{Managing experiments}
For managing the steps described further, such as training models, we need some overaching system -- steps variously fail, don't compile, don't fit in memory, etc., plus we would like to reuse partial results in more experiments.

Moses itself has built-in perl-based experiment managment system, called prosaically Experiment Management System (EMS). However, this system is not very widely used in on UFAL.

Instead of EMS, we use instead another perl-based tool called eman (experiment manager). Eman is described well in \cite{eman} or at its website, \url{http://ufal.mff.cuni.cz/eman}.  

Eman breaks down experiment into so-called \uv{steps}. Step encapsulates an atomic part of an experiment and can be in one of a few various states. More importantly, step can be dependent on various other states; if a step fails, all steps dependent on it automatically fail. The whole experiment is then just another step, dependent on all the necessary substeps.

Step is represented by a directory in a playground directory. Step is created by copying a script, called \uv{seed}, from a library of seeds, to a new directory.

In our opinion, eman itself is well written, while we found the seeds themselves hard to read, too repetitive, and with large amount of code copied and pasted over. We tried to rewrite the seeds as perl modules, so the reusability is higher.

I am not personally sure if this effort was successful. We decided to use MooseX::Declare, which seemed to us like a modern way to write modules in perl. 

Unfortunately, MooseX::Declare is using very difficult-to-understand perl concepts and source code transformations, and as a result, it takes long to run and, perhaps worse, returns very confusing and undecypherable errors. So as a result of our refractoring, we have seeds that have code that's probably easier to read and refractor, but on the other hand, it's slow and produces very opaque errors.

\subsection{Word alignment}
For word alignment, we are using MGIZA++\footnote{See \cite{mgiza}}, which is a GPL toolkit based on GIZA++\footnote{See \cite{giza}}, which is itself based on models, sometimes called IBM Model 1 to IBM Model 5\footnote{See \cite{ibm}}, which is itself based on expectation–maximization algorithm (EM).

IBM Models and the underlying EM algorithms are explained perfectly in Chapter~4 of \cite{koehn2010statistical} or in those slides by the same author -- \url{http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/ibm-model1.pdf}.

GIZA++ is an implementation of those models. MGIZA++ is just its multi-threaded variant, which makes the word alignment slightly faster.

\subsection{Phrase-extraction}
In this step, Moses takes the word alignment from the previous step and learns a so-called \uv{phrase table}.
Unlike word alignment, phrase extraction spans multiple words on every side in so-called \uv{phrases}.

Phrase table consists of list of phrases, their probabilities in both ways of translation, and their lexical weighting -- lexical weighting is the probability of the translated phrase counted by individual word pairs. The exact meaning of the numbers is well explained in \cite{koehn2003}.

The phrase-table - defines a so-called \uv{translation model}. 

\subsection{Language model}
Language model is a part of the system, that tries to model the probability of a target language sentence alone. It's trained on a monolingual corpus.

We use SRILM, which is an open source language modeling toolkit. (Although it's open-source, it uses its own license, that allows free use only for non-commercial and educational purposes.) Current status of SRILM is described in \cite{srilm}, original design is described in \cite{srilm_old}. 

SRILM uses several models, one of them is n-gram word model, described well in \cite{koehn2010statistical}\footnote{chapter 7}. We use n-grams model to the order 3 with words and order 5 with tags (see ???). We smooth the models with Kreser-Ney smoothing with Chen and Goodman's modification\footnote{See \cite{chen} and \url{http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html}}. 


\subsection{Language model interpolation}

We had more than one monolingual Russian corpora, but we weren't sure of how high quality each of them were and how helpful it would be. For this reason, we used so-call interpolation (also called mixing).

Linear interpolation in general is described for example in \cite{gutkin}. On a separate heldout data, set of \emph{lambdas} are trained -- the resulting probabilities are then just the individual probabilities, multiplied by the lambdas and summed.

Linear interpolation is supported by undocumented script in Moses \texttt{interpolate-lm.perl}, which in turn uses SRILM's undocumented AWK script \texttt{compute-best-mix.gawk} and SRILM's \texttt{ngram} with \texttt{-mix-lm} option\footnote{See \url{http://www.speech.sri.com/projects/srilm/manpages/ngram.1.html}}.

We used a linear interpolation instead of log-linear interpolation simply because we didn't notice the option until later in the project.

\subsection{Factored translation}
\subsubsection{Overview}

The pipeline, described in the previous section, translates phrases from the source language directly to the target language.

However, with morphologically rich languages such as Russian or Czech, this can result in too low results. With so-called factored translation, we can add some morphological information while still keeping the main ideas of phrase-based translation.

With factored translation, we add some morphological (or other) information to either source, target or both -- for example, lemma or morphological tag -- this is called a \emph{factor}. Then, instead of making language models and/or translation models on the words alone, we train them on some combination of these factors and then somehow put them together.

\subsubsection{Factor experiments}
In a separate set of experiments, that predate the whole experiment described here, only on the UMC data described in the section ??, we compared several set-ups for factored translation.

We used a set-up described for example in \cite{backoff}. However, since we do not have Russian morhpology fully working, we used only the system described as \texttt{lemma backoff} -- with the exception of not translating to lemma. We were not using interpolated backoff, simply because regular backoff is easier to use with Moses.

The main model translates from a word form on the source side to word form and tag on the target side. The backoff model translates from a lemma (or a stem -- see below) to form and tag on the target side.

For Russian, we used TreeTagger software\footnote{\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/}, also see \cite{treetagger1} and \cite{treetagger2}} with a Russian parameter file\footnote{trained on a corpus created by Serge Sharoff, see \url{http://corpus.leeds.ac.uk/mocky/}}. TreeTagger is a closed-source software with a restrictive license, but for free for research purposes.

For Czech, we used tokenizer from UFAL project Treex (described further in section XXX) and morphological analyzer Morče (\url{http://ufal.mff.cuni.cz/morce/references.php}); however, we didn't actually use its output, as described further.

When we experimented further, we found out that using not lemma on the source side, but a very crude lexical stem -- just using the first 4 letters of a word -- gets better results\footnote{Using stems instead of lemmas is suggested for example in XX. However, their stems are more linguistically motivated, while we just crudely take first few letter}. When we tried the experiment with different lengths of the stem, we got the results, shown in the graph XX -- the lexical stem of the lengths XX was the best.

Therefore, we continued to use that in our further experiments, as for example in the WMT submissions\footnote{citace tu}.




???? výsledky, ale stejně to budu přehazovat









