\chapter{Experiment setting}
\section{Statistical vs. rule-based translation systems}

Machine translation (MT) systems has historically used many different approaches. One way of classifying the approaches is on the axis of rule-based vs. statistical.

In general, we can re-use the descriptions, used by \cite{bojar}, which is as follows:
\begin{itemize}
\item rule based MT systems:
\begin{itemize}
\item use analysis, transfer and synthesis steps
\item use formal grammars
\item use hand-made dictionaries
\item have linguistic information hard-coded and therefore aren't language-agnostic
\end{itemize}
\item statistical MT systems
\begin{itemize}
    \item use more variants of outputs, rank them with some score, and choose the best one
    \item train internal dictionaries from big parallel data
    \item have more compact translation core, their inner working are less obvious
    \item use statistics instead of linguistic rules and therefore are more language-agnostic
\end{itemize}
\end{itemize}

However, with some of the modern systems, the distinction is not as clear-cut as we would like, for the purpose of our comparison. 
For example, statistical MT systems like Moses can get significantly better with some added linguistic information; 
on the other hand, systems like TectoMT, which can sometimes be classified as more rule-based, actually have big modules more or less based on statistics.

%In this thesis, I will present several existing solutions to Czech-Russian machine translation, describe their position on the aforementioned axis, present their history and discuss their results on a testing data.
\section{Development and testing data}

To compare several machine translation systems, we need some consistent data to show the deficiencies and/or improvements of different MT systems.

In general, we want to produce a system that's general enough and performs well on unseen sentences; for that reason, we should have -- in the case of statistical MT systems -- separated test set and training/development set.\footnote{As noted in for example \cite{koehn2010statistical}, p. 274, or \cite{bishop}, pg. 6}

We have tried to do that in our experiments with our data models. However, some systems evaluated in this thesis are only black-box systems (Google Translate, PC Translator). Especially with Google Translate, we cannot rule out the possibility that they already use some parts of the test/development data.


In general, we use two sets with two different domains - WMT data and Intercorp data.

\subsection{WMT}
One of the sets is the WMT test data. 
WMT (short for Workshop on Statistical Machine Translation) is an annual workshop, where various teams compete on a shared translation task with a shared test data.\footnote{See for example \cite{wmt_findings_2013}, or the rich history on \url{http://www.statmt.org}.}

As noted in \cite{wmt_findings_2013}, in 2013, Russian was added as one of the languages; it was still included in 2014. In both these years, the available data were divided into a test set and a development/training set.

The sentences in the training set are manually translated; for the year 2013, the sources are described in \cite{wmt_findings_2013}; for the year 2014, they are not yet formally described, but from an overview we can tell they used news sources again. The set contains both Czech and Russian.


Most Czech and Russian sentences in these sets are not a direct translation of each other though\footnote{Again, described in \cite{wmt_findings_2013}}; they are actually different translations of the same source sentences from various languages. 

Some of the source languages are Czech or Russian, so for those cases the data are indeed direct translations, but if we used only those sentences, the data would be significantly smaller.

However, if there is a different source language for both Czech and Russian side, the advantage of similarity of these two languages is lost -- we can actually see completely different language constructs and word order on Czech and Russian side, even when similar constructs and word order would be used when translation directly from Czech to Russian.

We extracted pairs of Czech and Russian from the test dataset; this dataset is 3000 sentences long for the year 2013 (WMT2013) and ??? sentences for the year 2014 (WMT2014). We use WMT2014 as a final test set, and WMT2013 as a development set, for fine-tuning the results.


\subsection{Intercorp}

Intercorp\footnote{Sometimes written as InterCorp} is a parallel corpus for many language pairs, including Czech and Russian. The history and other information is thoroughly described in \cite{intercorp}. 

The data itself is organized by source, and each data source is given an information of the original language, from which a given source is translated. We were able to extract just the data, that are either direct translations from Russian to Czech or vice versa, thanks to this information.

We also removed all the data, used in as a training data of our Moses model (see section ??)\footnote{this might sound illogical and backwards, but it's mainly because we built the Moses system first as a part of a different research with data from an older version of Intercorp corpus. See \cite{mujpaper}}. The resulting data is purely fictional novels, except for Jiří Levý's Art of Translation, which is a translation theory book. Interestingly, this is also the only book that has been translated from Czech to Russian and not the other way.

\jednatabulkan{icorpdata} { |l|l |l | l | }
{
         \hline
\textbf{Author}
&
\textbf{English name}
&
\textbf{Year}
&
\textbf{Sent.}

\\ \hline
Nikolai Ostrovsky &
How the Steel Was Tempered &
1936 &
9844
\\ \hline
Ilya Ilf, Yevgeni Petrov &
The Twelve Chairs &
1928 &
8525

\\ \hline

Mikhail Bulgakov &
The Master and Margarita &
1967 &
7124 
\\ \hline

Nikolai Nikolaevich Nosov &
 The Adventures of Neznaika and His Friends 
&1953-1954 &
3523




\\ \hline

Jiří Levý &
The Art of Translation &
1957 &
3149

\\ \hline

Aleksandr Solzhenitsyn
&
One Day in the Life of Ivan Denisovich
&
1962
&
3090

\\ \hline

Alexander Pushkin &
The Captain's Daughter 
&1863&
2984 
\\ \hline

Aleksandr Solzhenitsyn &
An Incident at Krechetovka Station &
1963&
1467 

\\ \hline
Aleksandr Solzhenitsyn &
Matryona's Place  
&1963
&
880

\\ \hline

} {Intercorp data} 


All the used novels are in the Table~\ref{tabulka:icorpdata}, \footnote{English transcriptions, English title translations and years of publication are taken from English wikipedia.}
sorted by the sentenced count.

As the reader can probably see, this dataset is markedly different from the first dataset. The data are bigger and are translated directly, on the other hand, the youngest book is from 1967 and the language itself is -- as a prosaic text -- harder to translate in general. Because the two corpora are too different, we did not try to join the WMT and the Intercorp sets.

From this set, we randomly selected 3000 sentences for final testing (IC-test); the rest is used as a development set (IC-dev).



\section{Experiments overview}
\label{sec:experiments}
\jednatabulkam{randsent1} { |X|X|X | }
{
\hline



Je mi teprve čtyřiadvacet let a nemohu prožit celý svůj život s legitimací invalidy práce a potloukat se po nemocnicích , když vím , že je to marné .   &   Мне всего двадцать четыре года , и я не могу доживать свой век с книжечкой инвалида труда , скитаться по лечебницам , зная , что это ни к чему .   &   Mne vsego dvadczat` chety're goda , i ya ne mogu dozhivat` svoj vek s knizhechkoj invalida truda , skitat`sya po lechebniczam , znaya , chto e`to ni k chemu . \\ \hline
Generál chodil po pokoji sem a tam , kouře svou pěnovku .   &   Генерал ходил взад и вперед по комнате , куря свою пенковую трубку .   &   General xodil vzad i vpered po komnate , kurya svoyu penkovuyu trubku . \\ \hline
" Možná že žádné brilianty neexistují ? "   &   - Может быть , никаких брильянтов нет ?   &   - Mozhet by't` , nikakix bril`yantov net ? \\ \hline
Nesměl při tom udělat chybu , vyžadovalo to stejnou přesnost , jako když se zaměřuje dělo .   &   Эта работа не допускала описки - так же как прицел орудия .   &   E`ta rabota ne dopuskala opiski - tak zhe kak pricel orudiya . \\ \hline
S hrdostí vzpomněl , jak snadno dobyl kdysi srdce krásné Heleny Baurové .   &   Он с гордостью вспомнил , как легко покорил когда-то сердце прекрасной Елены Боур .   &   On s gordost`yu vspomnil , kak legko pokoril kogda-to serdce prekrasnoj Eleny' Bour . \\ \hline





}{Random sentences and translations -- Intercorp}

\jednatabulkam{randsent2} { |X|X|X | }
{
\hline

Thiago Silva, který patří k nejlepším obráncům na světě, taky umožňuje ostatním vedle sebe růst.   &   Тьяго Силва, который является одним из лучших защитников в мире, также мотивирует всех двигаться вперед.   &   T`yago Silva, kotory'j yavlyaetsya odnim iz luchshix zashhitnikov v mire, takzhe motiviruet vsex dvigat`sya vpered. \\ \hline
"Dávali mi pět let života a už je to sedm," říká bez emocí na svém lůžku v domě pro paliativní péči Victor-Gadbois v Beloeil, kam přijel předešlý den.   &   "Мне давали пять лет, я прожил семь", - говорит он, между жизнью и смертью, лежа в кровати в приюте паллиативного ухода Виктор-Гадбуа в Белёй, куда прибыл накануне.   &   "Mne davali pyat` let, ya prozhil sem`", - govorit on, mezhdu zhizn`yu i smert`yu, lezha v krovati v priyute palliativnogo uxoda Viktor-Gadbua v Belyoj, kuda priby'l nakanune. \\ \hline
Opatrnost je ovšem na místě například na některých přemostěních, kde může být povrch namrzlý a kluzký.   &   Однако внимательность нужна, например, на мостах, где поверхность может быть намерзшая и скользкая.   &   Odnako vnimatel`nost` nuzhna, naprimer, na mostax, gde poverxnost` mozhet by't` namerzshaya i skol`zkaya. \\ \hline
Prostě je ignoruji.   &   Я просто не обращаю внимания.   &   Ya prosto ne obrashhayu vnimaniya. \\ \hline
Podle doktorky Christiane Martelové není quebecký zdravotnický systém dostatečně výkonný, aby zajistil přístup všech osob ke kvalitní paliativní péči, než bude možno souhlasit s provedením eutanazie.   &   По словам доктора Кристиан Мартель, система здравоохранения Квебека недостаточно эффективна, чтобы обеспечить право на паллиативный уход высокого качества до того, как будет разрешен переход к эвтаназии.   &   Po slovam doktora Kristian Martel`, sistema zdravooxraneniya Kvebeka nedostatochno e`ffektivna, chtoby' obespechit` pravo na palliativny'j uxod vy'sokogo kachestva do togo, kak budet razreshen perexod k e`vtanazii. \\ \hline


}{Random sentences and translations -- WMT2013}

For every translation system described, we tried to translate the same two development sets, described in the section above - WMT2013 and IC-dev. We describe the results on the sets, with a BLEU metric.

For illustration purposes, we have randomly selected 5 sentences from IC-dev and 5 from WMT-2013; the results are shown in Tables~\ref{tabulka:randsent1} and \ref{tabulka:randsent2}. 
On those, I want to demonstrate the conrete results of various machine translation systems. 
%I am also describing some common mistakes with selected sentences, demonstrating those mistakes.

I am presenting all Russian sentences with both original Cyrillic and GOST 7.79 RUS transliteration\footnote{See \cite{gost}}, for the convenience of the reader.


\subsection{Baseline}
\jednatabulkan{bleubase} { |r|r|r | }
{
\hline
&
intercorp&
WMT\\ \hline
BLEU & 0.77\% $\pm$ 0.06\%
&
0.83\% $\pm$ 0.16\%

\\ \hline
}{Baseline BLEU}

As a baseline for the translation, I use an automatic transliteration between Czech and Russian, mentioned above; to make transliteration possible, we also remove Czech diacritics.

Resulting BLEU is very low, as expected.
